{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tika\nimport tensorflow\nimport pandas as pd\nimport keras\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom spacy.pipeline import EntityRuler \n#from wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nstopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nlemmer = WordNetLemmatizer()\nimport tensorflow as tf\n#from tensorflow import keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.layers import Conv1D, LSTM , Dense, BatchNormalization, Input, Bidirectional, Dropout\nfrom keras.models import Model\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n#import nolds\nimport scipy\n#import pyeeg\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport tensorflow\nfrom keras.layers import Lambda, Dot, Concatenate, Activation, Embedding, add, Conv1D,GlobalMaxPool1D\nfrom keras.models import Sequential\nimport pickle\nimport tempfile\nfrom scipy import signal\nfrom mne.time_frequency import psd_array_welch\n#from tf.keras.models import Sequential, load_model, save_model, Mode\n%matplotlib inline\nimport sklearn\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom gensim import corpora\nimport gensim\nimport re\n#spacy\nimport spacy\nfrom spacy.pipeline import EntityRuler\nfrom spacy.lang.en import English\nfrom spacy.tokens import Doc\nimport numpy as np\nfrom tika import parser","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:29:28.670934Z","iopub.execute_input":"2022-03-07T06:29:28.671401Z","iopub.status.idle":"2022-03-07T06:29:55.665279Z","shell.execute_reply.started":"2022-03-07T06:29:28.671311Z","shell.execute_reply":"2022-03-07T06:29:55.664302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### dataset1","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/jobsextractor/Resume.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:30:32.173613Z","iopub.execute_input":"2022-03-07T06:30:32.17459Z","iopub.status.idle":"2022-03-07T06:30:33.477697Z","shell.execute_reply.started":"2022-03-07T06:30:32.174541Z","shell.execute_reply":"2022-03-07T06:30:33.476913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:30:33.479406Z","iopub.execute_input":"2022-03-07T06:30:33.479872Z","iopub.status.idle":"2022-03-07T06:30:33.506303Z","shell.execute_reply.started":"2022-03-07T06:30:33.479831Z","shell.execute_reply":"2022-03-07T06:30:33.505442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data['Category'].value_counts()\nimport re\nclean = []\nfor i in range(data.shape[0]):\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        data[\"Resume_str\"].iloc[i],\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)\n    clean.append(review)\ndata['cleaned-1'] = clean","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:30:35.949411Z","iopub.execute_input":"2022-03-07T06:30:35.949743Z","iopub.status.idle":"2022-03-07T06:30:58.343612Z","shell.execute_reply.started":"2022-03-07T06:30:35.94971Z","shell.execute_reply":"2022-03-07T06:30:58.342932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data = data.reindex(np.random.permutation(data.index))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:14:29.001698Z","iopub.execute_input":"2021-12-11T03:14:29.002167Z","iopub.status.idle":"2021-12-11T03:14:29.007598Z","shell.execute_reply.started":"2021-12-11T03:14:29.002123Z","shell.execute_reply":"2021-12-11T03:14:29.006882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### dataset2","metadata":{}},{"cell_type":"code","source":"import csv\ntsv_file = open(\"/kaggle/input/jobsextractor/train.tsv\")\nread_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\njobs_description = []\nfor row in read_tsv:\n      jobs_description.append(row)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:30:26.186577Z","iopub.execute_input":"2022-03-07T06:30:26.187179Z","iopub.status.idle":"2022-03-07T06:30:26.494859Z","shell.execute_reply.started":"2022-03-07T06:30:26.187104Z","shell.execute_reply":"2022-03-07T06:30:26.49396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### dataset3","metadata":{}},{"cell_type":"code","source":"jobs_description2 = pd.read_csv(\"/kaggle/input/jobsextractor/dice_com-job_us_sample.csv\")\ndescriptions_jobs = jobs_description2['jobdescription']","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:30:58.345426Z","iopub.execute_input":"2022-03-07T06:30:58.345668Z","iopub.status.idle":"2022-03-07T06:30:59.799514Z","shell.execute_reply.started":"2022-03-07T06:30:58.345635Z","shell.execute_reply":"2022-03-07T06:30:59.79856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path  = \"/kaggle/input/jobsextractor/yashi.pdf\"\nspacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\nclean_descriptions = []\nfor i in descriptions_jobs:\n    clean_descriptions.append(spacy_.cleaning_texts(i))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:11:35.357777Z","iopub.execute_input":"2022-03-07T04:11:35.358198Z","iopub.status.idle":"2022-03-07T04:11:35.845984Z","shell.execute_reply.started":"2022-03-07T04:11:35.358151Z","shell.execute_reply":"2022-03-07T04:11:35.844801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(clean_descriptions)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:11:35.847036Z","iopub.status.idle":"2022-03-07T04:11:35.847371Z","shell.execute_reply.started":"2022-03-07T04:11:35.847201Z","shell.execute_reply":"2022-03-07T04:11:35.847218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs = clean_descriptions[:1000]\ndictionary = corpora.Dictionary(d.split() for d in docs)\nbow = [dictionary.doc2bow(d.split()) for d in docs]\nlda = gensim.models.ldamodel.LdaModel\nnum_topics = 10\nldamodel = lda(\n    bow, \n    num_topics=num_topics, \n    id2word=dictionary, \n    passes=50, \n    minimum_probability=0\n)\n#ldamodel.print_topics(num_topics=num_topics)\n\ncommon_words = []\nfor index, topic in ldamodel.show_topics(formatted=False, num_words= 30):\n        for w in topic:\n            common_words.append(w[0])\ncommon_word = \" \".join([i for i in common_words])\nwith open(\"/kaggle/working/lda_words.txt\", 'w') as f:\n    f.write(common_word)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:00:11.656146Z","iopub.execute_input":"2021-12-10T08:00:11.656457Z","iopub.status.idle":"2021-12-10T08:02:36.926081Z","shell.execute_reply.started":"2021-12-10T08:00:11.656424Z","shell.execute_reply":"2021-12-10T08:02:36.925074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/input/jobsextractor/j.txt\")\ntxt = f.read()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:16:31.702119Z","iopub.execute_input":"2021-12-10T08:16:31.702548Z","iopub.status.idle":"2021-12-10T08:16:31.709893Z","shell.execute_reply.started":"2021-12-10T08:16:31.702509Z","shell.execute_reply":"2021-12-10T08:16:31.708889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/input/jobsextractor/j.txt\")\ntxt = f.read()\n\nwords_list2 = []\nc = txt.split(\"-\")\nfor i in c:\n    t = i.split('\\n')\n    if len(t) == 2:\n        words_list2.append(t[1])\n\nfinal_words_list2 = []\nfor i in words_list2:\n    #print(i)\n    final_words_list2.append(spacy_.cleaning_texts(i.split(\".\")[1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:25:03.739578Z","iopub.execute_input":"2021-12-11T03:25:03.741233Z","iopub.status.idle":"2021-12-11T03:25:03.76721Z","shell.execute_reply.started":"2021-12-11T03:25:03.741158Z","shell.execute_reply":"2021-12-11T03:25:03.766399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r'\\b(?:total|staff)\\b'\npatterns3= []\nfor i in final_words_list2:\n    patterns3.append(r'\\b(?:'+str(i)+')\\b')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:30:06.837396Z","iopub.execute_input":"2021-12-11T03:30:06.83777Z","iopub.status.idle":"2021-12-11T03:30:06.844884Z","shell.execute_reply.started":"2021-12-11T03:30:06.837729Z","shell.execute_reply":"2021-12-11T03:30:06.843473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_words_list2\n#patterns3","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:59:40.239745Z","iopub.execute_input":"2021-12-11T03:59:40.240336Z","iopub.status.idle":"2021-12-11T03:59:40.245995Z","shell.execute_reply.started":"2021-12-11T03:59:40.240083Z","shell.execute_reply":"2021-12-11T03:59:40.244964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### dataset4\n","metadata":{}},{"cell_type":"code","source":"data_science_dataset = pd.read_csv(\"/kaggle/input/jobsextractor/DATA_SCIENCE.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:31:15.556729Z","iopub.execute_input":"2022-03-07T06:31:15.557124Z","iopub.status.idle":"2022-03-07T06:31:16.832252Z","shell.execute_reply.started":"2022-03-07T06:31:15.557084Z","shell.execute_reply":"2022-03-07T06:31:16.831132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_science_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:11:51.138595Z","iopub.execute_input":"2022-03-07T04:11:51.138894Z","iopub.status.idle":"2022-03-07T04:11:51.170283Z","shell.execute_reply.started":"2022-03-07T04:11:51.13886Z","shell.execute_reply":"2022-03-07T04:11:51.169271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spacy_1 =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\ndocs = data_science_dataset['job_description'][:1000]\ntexts = []\nfor i in docs:\n    texts.append(spacy_1.cleaning_texts(i))\ndictionary = corpora.Dictionary(d.split() for d in texts)\nbow = [dictionary.doc2bow(d.split()) for d in texts]\nlda = gensim.models.ldamodel.LdaModel\nnum_topics = 30\nldamodel = lda(\n    bow, \n    num_topics=num_topics, \n    id2word=dictionary, \n    passes=50, \n    minimum_probability=0\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:12:12.355972Z","iopub.execute_input":"2022-03-07T04:12:12.356782Z","iopub.status.idle":"2022-03-07T04:16:19.333981Z","shell.execute_reply.started":"2022-03-07T04:12:12.356748Z","shell.execute_reply":"2022-03-07T04:16:19.332725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ldamodel.print_topics(num_topics=num_topics)\ncommon_words2 = []\nfor index, topic in ldamodel.show_topics(formatted=False, num_words= 100):\n        for w in topic:\n            common_words2.append(w[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:16:23.417812Z","iopub.execute_input":"2022-03-07T04:16:23.418658Z","iopub.status.idle":"2022-03-07T04:16:23.448961Z","shell.execute_reply.started":"2022-03-07T04:16:23.418615Z","shell.execute_reply":"2022-03-07T04:16:23.44808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#common_words2 ","metadata":{"execution":{"iopub.status.busy":"2021-12-12T01:35:08.950903Z","iopub.execute_input":"2021-12-12T01:35:08.951773Z","iopub.status.idle":"2021-12-12T01:35:08.955382Z","shell.execute_reply.started":"2021-12-12T01:35:08.951719Z","shell.execute_reply":"2021-12-12T01:35:08.954488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = English()\nfs = []\nfor i in range(len(descriptions_jobs )):\n    my_doc = nlp(descriptions_jobs[i])\n\n    token_list = []\n    for token in my_doc:\n        token_list.append(token.text)\n\n    filtered_sentence =[] \n\n    for word in token_list:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_sentence.append(word) \n    for i in filtered_sentence:\n        fs.append(i)\n    #print(token_list)\n    #print(filtered_sentence) \n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-10T05:38:48.870977Z","iopub.execute_input":"2021-12-10T05:38:48.87127Z","iopub.status.idle":"2021-12-10T05:39:43.8268Z","shell.execute_reply.started":"2021-12-10T05:38:48.871238Z","shell.execute_reply":"2021-12-10T05:39:43.826113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#t = jobs_description[11][1]\n#def removearticles(text):\n#    t2 = re.sub('\\s+(a|an|and|the)(\\s+)', '\\2', text)\n#    print(t2)\n    \n#removearticles(t)\n#c = nlp(descriptions_jobs[0])\n#for i in c.ents:\n #   print(i.text)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:44:42.672668Z","iopub.execute_input":"2021-12-10T08:44:42.672981Z","iopub.status.idle":"2021-12-10T08:44:42.678137Z","shell.execute_reply.started":"2021-12-10T08:44:42.672948Z","shell.execute_reply":"2021-12-10T08:44:42.677385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing pipeline","metadata":{}},{"cell_type":"code","source":"class resume_spacy_pdf_clean_skills():\n    \n    def __init__(self, path_to_pdf, cleaning_type):\n        self.path = path_to_pdf\n        #clean_types = [\"mycleaning\", \"specific_cleaning\"]\n        self.cleaning_type = cleaning_type\n        \n        \n    def nlp_model_initalization(self):\n        nlp = spacy.load(\"en_core_web_lg\")\n        ruler = nlp.add_pipe(\"entity_ruler\")\n        ruler.from_disk(\"/kaggle/input/jobsextractor/jobs2.jsonl\")\n        #ruler.from_disk(\"/kaggle/input/jobsextractor/skills_12_dec_2.json\")\n        return nlp, ruler\n    \n    def pdf_to_text(self):\n        raw = parser.from_file(self.path)\n        text = raw['content']\n        return text\n    \n    def cleaning_texts(self, text):\n        if self.cleaning_type == \"my_cleaning\":\n            resumeText = text\n            resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n            resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n            resumeText = re.sub('#S+', '', resumeText)  # remove hashtags\n            resumeText = re.sub('@S+', '  ', resumeText)  # remove mentions\n            resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n            resumeText = re.sub(r'[^x00-x7f]',r' ', resumeText) \n            resumeText = re.sub('s+', ' ', resumeText)  # remove extra whitespace\n            text = resumeText\n            text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n            tokens = re.split('\\W+', text)\n            text = [lemmer.lemmatize(word) for word in tokens if word not in stopwords]\n            review  = \" \".join(i for i in text)\n        \n        if self.cleaning_type == 'specific_cleaning':\n\n                review = re.sub(\n                    '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n                    \" \",\n                    text,\n                )\n                review = review.lower()\n                review = review.split()\n                lm = WordNetLemmatizer()\n                review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n                review = \" \".join(review)\n        return review\n\n    def get_skills(self, nlp, text):\n        doc = nlp(text)\n        skills_ = []\n        others = []\n        for ent in doc.ents:\n            if \"SKILL\" in ent.label_:\n                skills_.append(ent.text)\n            elif ent.label_ == \"ORG\":\n                others.append(ent.text)\n         \n        return skills_, others\n    \n    def get_job_resume_discription(self,text, pattern):\n        ans = []\n        pattern2 = pattern\n        sp = text.split(\"\\n\")\n        if len(sp) <=3 : \n            text2 = text.split(\"\\xa0\")\n            for pat in pattern2:\n                for t in text2:\n                     if re.search(pat, t) != None:\n                            ans.append(t)\n        else:\n            for pat in pattern2:\n                for t in text.split('\\n'):\n                     if re.search(pat, t) != None:\n                            ans.append(t)\n\n        ans2 = \" \".join([i for i in list(set(ans))])\n        #final  = clean_data(ans2)\n        return ans2 , list(set(ans))\n    \n    def get_description_without_re(self, text, pattern):\n        sent = []\n        text2 = inp.split('\\n')\n        #p5_ = p5.split(\" \")+p6.split(\" \")\n        for i in p5.split(\" \"):\n            for j in text2:\n                if i.lower() in j.lower() and i not in sent:\n                    sent.append(j)\n                    #print(\"Text:-{}, pat:-{}\".format(j,i))\n       \n        \n        return sent\n                \n\n    def get_create_patterns(self, text):\n        \n        pattern2 = [r'\\b(?i)'+'plan'+r'\\b', r'\\b(?i)'+'years'+r'\\b',\n        r'\\b(?i)'+'experience'+r'\\b',\n        r'\\b(?i)'+'worked'+r'\\b',\n        r'\\b(?i)'+'willing'+r'\\b',\n        r'\\b(?i)'+'knowledge'+r'\\b',\n        r'\\b(?i)'+'interview'+r'\\b', \n        r'\\b(?i)'+'applicants'+r'\\b',\n        r'\\b(?i)'+'interview'+r'\\b',\n        r'\\b(?i)'+'immediate'+r'\\b',\n        r'\\b(?i)'+'interested'+r'\\b',\n        r'\\b(?i)'+'opening'+r'\\b',]\n\n        #for i in w:\n         #           pattern2.append(r'\\b(?i)'+ str(i) + r'\\b')\n        w2 = text.split(\" \")\n        for i in w2:\n            pattern2.append(r'\\b(?i)'+str(i)+r'\\b')\n    \n        return pattern2\n    \n    def get_description_skill(self,nlp,des):\n        skill = []\n        des = des.lower()\n        d = nlp(des)\n        for i in d.ents:\n            if 'SKILL' in i.label_:\n                skill.append(i.text)\n        return set(skill)\n    \n    def get_salary(self, text_from_pdf):\n        pat2 = [r'\\b(?i)'+'salary'+r'\\b', r'\\b(?i)'+'Rs'+r'\\b', r'\\b(?i)'+'rs'+r'\\b']\n\n        sal = []\n        for p in pat2:\n            for i in text_from_pdf.lower().split(\"\\n\"):\n                if re.search(p, i)!= None:\n                    sal.append(i)\n        return sal\n    \n    \n    \n    #b =\"posting for a computer engineer job in microsoft\"\n    #v = b.index(\"engineer\")\n    def get_job_from_training_spacy_model(self, data,nlp, clean_data):\n    ## Making data for training\n        TRAIN_DATA = data\n\n\n        ##Loading model from NLP\n\n        LABEL = \"JOB\"\n        nlp, ruler = spacy_.nlp_model_initalization()\n        pipes1 = nlp.pipe_names\n        ner=nlp.get_pipe(\"ner\")\n        optimizer = nlp.resume_training()\n        move_names = list(ner.move_names)\n        #pipe_exceptions = pipes1\n        pipe_exceptions = [\"ner\", \"tagger\", \"tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        #pipe_exceptions = [\"ner\", \"tagger\", \"tok2vec\"]\n        \n        for _, annotations in TRAIN_DATA:\n            for ent in annotations.get(\"entities\"):\n                ner.add_label(ent[2])\n\n\n\n        ### Training the model\n        # Import requirements\n        import random\n        from spacy.util import minibatch, compounding\n        from pathlib import Path\n        from spacy.training.example import Example\n\n        # TRAINING THE MODEL\n        with nlp.disable_pipes(*other_pipes):\n\n          # Training for 30 iterations\n          for iteration in range(30):\n\n            # shuufling examples  before every iteration\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            #annotations = [entities for text, entities in batches]\n            for batch in batches:\n                    texts, annotations = zip(*batch)\n\n                    example = []\n                    # Update the model with iterating each text\n                    for i in range(len(texts)):\n                        doc = nlp.make_doc(texts[i])\n                        example.append(Example.from_dict(doc, annotations[i]))\n\n                    # Update the model\n                    nlp.update(example, drop=0.5, losses=losses)\n\n\n        ### Saving the model\n        from pathlib import Path\n        output_dir=Path('/kaggle/working/model')\n\n        # Saving the model to the output directory\n        if not output_dir.exists():\n              output_dir.mkdir()\n        nlp.meta['name'] = 'my_ner'  # rename model\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        job = []\n        text = nlp(clean_text.lower())\n        for i in text.ents:\n            if \"JOB\" in i.label_:\n                job.append(i.text)\n\n        return job\n    \n    def get_number_of_post(self, text_from_pdf):\n        post = []\n        pat2 = [r'\\b(?i)'+'senior'+r'\\b',r'\\b(?i)'+'Trainee'+r'\\b',r'\\b(?i)'+'post'+r'\\b',r'\\b(?i)'+'reserch fellow'+r'\\b',r'\\b(?i)'+'junior'+r'\\b',r'\\b(?i)'+'nos'+r'\\b', r'\\b(?i)'+'position'+r'\\b', r'\\b(?i)'+'required'+r'\\b', r'\\b(?i)'+'posting'+r'\\b', r'\\b(?i)'+'vocation'+r'\\b',  r'\\b(?i)'+'vacancy'+r'\\b',  r'\\b(?i)'+'opening'+r'\\b',  r'\\b(?i)'+'place'+r'\\b']\n        sal = []\n        for p in pat2:\n            for i in text_from_pdf.lower().split(\"\\n\"):\n                if re.search(p, i)!= None:\n                    post.append(i)\n\n        return post\n\n                \n    def get_matching_score(self,req, original):\n        req_skills = req\n        resume_skills = original\n        score = 0\n        for x in req_skills:\n            if x in resume_skills:\n                score += 1\n        req_skills_len = len(req_skills)\n        match = round(score / req_skills_len * 100, 1)\n\n        print(f\"The current Resume is {match}% matched to your requirements\")\n        return match \n\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:31:24.481333Z","iopub.execute_input":"2022-03-07T06:31:24.481678Z","iopub.status.idle":"2022-03-07T06:31:24.529039Z","shell.execute_reply.started":"2022-03-07T06:31:24.48164Z","shell.execute_reply":"2022-03-07T06:31:24.527619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\np = \"LEADERSHIP Accomplished Achieved Administered Analyzed Assigned Attained Chaired Consolidated Contracted Coordinated Delegated Developed Directed Earned Evaluated Executed Handled Headed Impacted Improved Increased Led Mastered Orchestrated Organized Oversaw Planned Predicted Prioritized Produced Proved Recommended  Regulated Reorganized Reviewed Scheduled Spearheaded Strengthened Supervised Surpassed  COMMUNICATION  Addressed Arbitrated Arranged Authored Collaborated Convinced Corresponded Delivered Developed Directed Documented Drafted Edited Energized Enlisted Formulated Influenced Interpreted Lectured Liaised Mediated Moderated Negotiated Persuaded Presented Promoted Publicized Reconciled Recruited Reported Rewrote Spoke Suggested Synthesized Translated Verbalized Wrote RESEARCH Clarified Collected Concluded Conducted Constructed Critiqued Derived Determined Diagnosed Discovered Evaluated Examined Extracted Formed Identified Inspected Interpreted Interviewed Investigated Modeled Organized Resolved Reviewed Summarized Surveyed Systematized Tested TECHNICAL Assembled Built Calculated Computed Designed Devised Engineered Fabricated Installed Maintained Operated Optimized Overhauled Programmed Remodeled Repaired Solved Standardized Streamlined Upgraded TEACHING Adapted Advised Clarified Coached Communicated Coordinated Demystified Developed Enabled Encouraged Evaluated Explained Facilitated Guided Informed Instructed Persuaded Set Goals Stimulated Studied Taught Trained QUANTITATIVE Administered Allocated Analyzed Appraised Audited Balanced Budgeted Calculated Computed Developed Forecasted Managed Marketed Maximized Minimized Planned Projected Researched CREATIVE Acted Composed Conceived Conceptualized Created Customized Designed Developed Directed Established Fashioned Founded Illustrated Initiated Instituted Integrated Introduced Invented Originated Performed Planned Published Redesigned Revised Revitalized Shaped Visualized HELPING Assessed Assisted Clarified Coached Counseled Demonstrated Diagnosed Educated\\ Enhanced Expedited Facilitated Familiarized Guided Motivated Participated Proposed Provided Referred Rehabilitated Represented Served Supported ORGANIZATIONAL\\ Approved Accelerated Added Arranged Broadened Cataloged Centralized Changed Classified Collected Compiled Completed Controlled Defined Dispatched Executed Expanded Gained Gathered Generated Implemented Inspected Launched Monitored Operated Organized Prepared Processed Purchased Recorded Reduced Reinforced Retrieved Screened Selected Simplified Sold Specified Steered Structured Systematized Tabulated Unified Updated Utilized Validated Verified\"\npatterns = \"Administer Advise Analyzes Approve Arranges Assesses Assigns Assists Attends Audits Authorizes Collaborate Collects1 Communicate Compile Conduct Confers Confirms Consolidates Consult Coordinates1 Counsel Create Delegate Deliver Designs Develop Direct Disseminates Distribute Documents1 Draft Edit Educate Establish Estimate Evaluate Examines Facilitates Formulate Gather Guide Implement Inform Initiates Integrates Interact Interpret Investigates Issue Maintains Manage Modifies Monitors Motivate Negotiate Obtain Order Organizes Oversees1 Participate Plan Prepares Present Processes1 Produces Provide Recommends Reconcile Records Recruit Research Responds Review Scans Schedules Searches Selects Serves Solicit Solve Submit Supervise Supply Test Train Translate Verifies\"\n#p2 = \" \".join([i for i in final_words_list2])+patterns+p","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:31:26.912988Z","iopub.execute_input":"2022-03-07T06:31:26.913996Z","iopub.status.idle":"2022-03-07T06:31:26.920907Z","shell.execute_reply.started":"2022-03-07T06:31:26.913939Z","shell.execute_reply":"2022-03-07T06:31:26.920144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/jobsextractor/working_words.txt\", 'r') as f:\n        patterns = f.read()\nwith open(\"/kaggle/input/jobsextractor/lda_words.txt\", 'r') as f1:\n        patterns2= f1.read()\n\nwith open(\"/kaggle/input/jobsextractor/j2.txt\", 'r') as f2:\n        file = f2.read()","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:31:27.106152Z","iopub.execute_input":"2022-03-07T06:31:27.106458Z","iopub.status.idle":"2022-03-07T06:31:27.114136Z","shell.execute_reply.started":"2022-03-07T06:31:27.106427Z","shell.execute_reply":"2022-03-07T06:31:27.113372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pattern4 = set(file.split(\"\\n\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:31:27.343455Z","iopub.execute_input":"2022-03-07T06:31:27.344508Z","iopub.status.idle":"2022-03-07T06:31:27.349503Z","shell.execute_reply.started":"2022-03-07T06:31:27.344456Z","shell.execute_reply":"2022-03-07T06:31:27.348503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = [\n              (\"Name of the Posts: Programmer \", {\"entities\": [(19, 29, \"JOB\")]}),\n              (\"Requirement for analyst part time in google\", {\"entities\": [(16, 23, \"JOB\")]}),\n              (\"Job posting for a writter\", {\"entities\": [(18, 25, \"JOB\")]}),\n              (\"vacancy for a manager in tata industries\", {\"entities\": [(14,21, \"JOB\")]}),\n              (\"posting for a intern in IIT bhu\", {\"entities\": [(14,20, \"JOB\")]}),\n              (\"vacancy for a research intern\", {\"entities\": [(14,22, \"JOB\")]}),\n              (\"required a technician for chemistry lab\", {\"entities\": [(11,21, \"JOB\")]}),\n              (\"temprary requirement for research fellow urgently\", {\"entities\": [(34,40, \"JOB\")]}),\n              (\"position for senior journslist in ABP News\", {\"entities\": [(20,30, \"JOB\")]}),\n              (\"employment for a engineer needed urgently\", {\"entities\": [(17,25, \"JOB\")]}),\n              (\"medical traineer at aiims delhi part time reqiured\", {\"entities\": [(8,16, \"JOB\")]}),\n              (\"post for a screwdriver endevour is empty from our neighbour\", {\"entities\": [(11,22, \"JOB\")]}),\n              (\"posting for a computer engineer job in microsoft\", {\"entities\": [(23,31, \"JOB\")]}),\n              (\"profession required is a manager in JSW\", {\"entities\": [(25,32, \"JOB\")]}),\n              (\"opening for a web developer in india\", {\"entities\": [(18,27, \"JOB\")]})\n              ]\n## p6 = working_2_10_dec, p5 = working_10_dec\np6 = [\"agile\", \"deadline-oriented\", \"multitask\", \"pressure\",\"multitasking\", \"enthusiastic\", \"high energy\", \"committed\", \"proactive\", \"pressure\", \"independently\", \"entrepreneurial\", \"independent\", \"resourceful\"]\np5 = \"essential salary necessary desirable applicant strong background qualification overtime experience worked knowlegde interview applicants immediate opening required flexible worked working skill skills role roles key full-time part-time well-paid badly paid high-powered stressful challenging rewarding repetitive glamorous plan years experience worked willing knowledge interview applicants interview immediate interested opening responsiblity resposiblities Administrative assistant Customer service Receptionist Part time UPS package handler part time entry level\"","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:31:27.864418Z","iopub.execute_input":"2022-03-07T06:31:27.864969Z","iopub.status.idle":"2022-03-07T06:31:27.878097Z","shell.execute_reply.started":"2022-03-07T06:31:27.864912Z","shell.execute_reply":"2022-03-07T06:31:27.877028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p51 = \"warehouse support Accounting Human resources Warehouse Sales Manager Data entry Administrative Retail Executive assistant Project manager Medical Assistant Marketing Accountant Cashier Registered nurse Business analyst Office IT Warehouse worker Office manager Finance Mechanical engineer Construction Entry level Clerical Controller Engineer Manufacturing Accounts payable Paralegal Forklift operator Customer service representative LPN Call center Graphic designer Information technology Office assistant Maintenance Full time Customer services representative Driver Operations manager Data analyst Part-time Nurse Security Healthcare Bookkeeper Remote Analyst Pharmacist RN Sales representative Management Welder Payroll Office clerk Supervisor Nurse practitioner Attorney Purchasing Recruiter Financial analyst Software engineer Director Logistics Sales manager Electrician Server Banking Delivery driver Medical office receptionist Truck driver Assistant Legal Warehouse manager Insurance Teacher Education Real estate Secretary Engineering Account manager Medical Production supervisor Bartender CnA Buyer Maintenance technician Graphic design Automotive Accounts receivable Security officer Restaurant\"","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:30:14.540011Z","iopub.execute_input":"2022-03-07T06:30:14.540485Z","iopub.status.idle":"2022-03-07T06:30:14.545451Z","shell.execute_reply.started":"2022-03-07T06:30:14.540434Z","shell.execute_reply":"2022-03-07T06:30:14.544566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#p51_ = []\n#for i in c.split(\" \"):\n#    p51_.append(r'\\b(?i)'+str(i)+r'\\b')","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:33:43.791905Z","iopub.execute_input":"2022-03-07T04:33:43.792715Z","iopub.status.idle":"2022-03-07T04:33:43.796494Z","shell.execute_reply.started":"2022-03-07T04:33:43.792672Z","shell.execute_reply":"2022-03-07T04:33:43.795524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = data_science_dataset['job_description'][5]\npath  =\"/kaggle/input/jobsextractor/harshit.pdf\"\nspacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\ntext_from_pdf = spacy_.pdf_to_text()\nclean_text = spacy_.cleaning_texts(text_from_pdf)\npattern = p5+ \"\".join([i for i in p6]) #\"\".join([i for i in common_words2]) \nnlp, ruler = spacy_.nlp_model_initalization()\npat = spacy_.get_create_patterns(pattern)\ndes , list_des = spacy_.get_job_resume_discription(text_from_pdf, pat)\nsal = spacy_.get_salary(text_from_pdf)\nskills_required = spacy_.get_description_skill(nlp, text_from_pdf)\nnumber_of_post = spacy_.get_number_of_post(text_from_pdf)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:34:02.129055Z","iopub.execute_input":"2022-03-07T06:34:02.12987Z","iopub.status.idle":"2022-03-07T06:34:04.954513Z","shell.execute_reply.started":"2022-03-07T06:34:02.129823Z","shell.execute_reply":"2022-03-07T06:34:04.95364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skills_required","metadata":{"execution":{"iopub.status.busy":"2022-03-07T06:34:10.854129Z","iopub.execute_input":"2022-03-07T06:34:10.854424Z","iopub.status.idle":"2022-03-07T06:34:10.85975Z","shell.execute_reply.started":"2022-03-07T06:34:10.854391Z","shell.execute_reply":"2022-03-07T06:34:10.859178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner = nlp.get_pipe(\"ner\")","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:42:52.599032Z","iopub.execute_input":"2022-03-07T04:42:52.599546Z","iopub.status.idle":"2022-03-07T04:42:52.604723Z","shell.execute_reply.started":"2022-03-07T04:42:52.599504Z","shell.execute_reply":"2022-03-07T04:42:52.603613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _, annotations in data:\n    for ent in annotations.get(\"entities\"):\n        ner.add_label(ent[2])","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:43:26.883814Z","iopub.execute_input":"2022-03-07T04:43:26.884122Z","iopub.status.idle":"2022-03-07T04:43:26.890293Z","shell.execute_reply.started":"2022-03-07T04:43:26.884091Z","shell.execute_reply":"2022-03-07T04:43:26.889469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"job = set(spacy_.get_job_from_training_spacy_model(data, nlp, text_from_pdf))","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:41:40.963505Z","iopub.execute_input":"2022-03-07T04:41:40.963807Z","iopub.status.idle":"2022-03-07T04:41:50.359528Z","shell.execute_reply.started":"2022-03-07T04:41:40.963778Z","shell.execute_reply":"2022-03-07T04:41:50.358704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom spacy.util import minibatch, compounding\nfrom pathlib import Path\n\n# TRAINING THE MODEL\nfor iteration in range(30):\n\n    # shuufling examples  before every iteration\n    random.shuffle(data)\n    losses = {}\n    # batch up the examples using spaCy's minibatch\n    batches = minibatch(data, size=compounding(4.0, 32.0, 1.001))\n    for batch in batches:\n        texts, annotations = zip(*batch)\n        nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n        print(\"Losses\", losses)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:44:42.649388Z","iopub.execute_input":"2022-03-07T04:44:42.65031Z","iopub.status.idle":"2022-03-07T04:44:42.692939Z","shell.execute_reply.started":"2022-03-07T04:44:42.650267Z","shell.execute_reply":"2022-03-07T04:44:42.692104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"des","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:20:51.276487Z","iopub.execute_input":"2022-03-07T04:20:51.277506Z","iopub.status.idle":"2022-03-07T04:20:51.285311Z","shell.execute_reply.started":"2022-03-07T04:20:51.27746Z","shell.execute_reply":"2022-03-07T04:20:51.284404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(text_from_pdf)","metadata":{"execution":{"iopub.status.busy":"2022-03-07T04:34:57.142316Z","iopub.execute_input":"2022-03-07T04:34:57.142606Z","iopub.status.idle":"2022-03-07T04:34:57.146522Z","shell.execute_reply.started":"2022-03-07T04:34:57.142579Z","shell.execute_reply":"2022-03-07T04:34:57.145712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_of_post","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:11:26.336248Z","iopub.execute_input":"2021-12-14T04:11:26.336497Z","iopub.status.idle":"2021-12-14T04:11:26.343721Z","shell.execute_reply.started":"2021-12-14T04:11:26.336466Z","shell.execute_reply":"2021-12-14T04:11:26.342648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c = \"position post place situation appointment posting work calling career trade capacity function occupation profession craft employment placement vocation pursuit activity billet field métier office opening station vacancy berth business connection job employ grip livelihood position employment engagement faculty field gig grind handicraft work nine-to-five racket spot line business earning \"\n#c.split(\" \")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T08:51:29.404355Z","iopub.execute_input":"2021-12-13T08:51:29.404629Z","iopub.status.idle":"2021-12-13T08:51:29.408221Z","shell.execute_reply.started":"2021-12-13T08:51:29.404603Z","shell.execute_reply":"2021-12-13T08:51:29.407716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(text_from_pdf)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T08:40:46.141597Z","iopub.execute_input":"2021-12-13T08:40:46.141849Z","iopub.status.idle":"2021-12-13T08:40:46.146545Z","shell.execute_reply.started":"2021-12-13T08:40:46.141814Z","shell.execute_reply":"2021-12-13T08:40:46.145337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith open(\"/kaggle/input/jobsextractor/skills_12-dec.txt\", 'r') as f:\n    skills_12_dec = f.read()\nskills_12_dec_1 = list(set(skills_12_dec.lower().split(\"\\n\")))\nskills_12_dec_2 = []\nfor i in skills_12_dec_1[1:]:\n        j = i.split(\" \")\n        if len(j) == 2:\n            skills_12_dec_2.append([{\"label\":\"SKILL|\"+str(i).replace(\" \",\"-\"),\"pattern\":[{\"LOWER\":str(j[0])},{\"LOWER\":str(j[1])}]}])\n        elif len(j) == 3:\n            skills_12_dec_2.append({\"label\":\"SKILL|\"+str(i).replace(\" \",\"-\"),\"pattern\":[{\"LOWER\":str(j[0])},{\"LOWER\":str(j[1])},{\"LOWER\":str(j[2])}]})\n        elif len(j) == 4:\n            skills_12_dec_2.append({\"label\":\"SKILL|\"+str(i).replace(\" \",\"-\"),\"pattern\":[{\"LOWER\":str(j[0])},{\"LOWER\":str(j[1])},{\"LOWER\":str(j[2])}, {\"LOWER\":str(j[3])}]})\n​\nfor i in skills_12_dec_2:\n    ruler.add_patterns([i])\n\n#skills_12_dec_2_ = \"/n\".join([str(i) for i in skills_12_dec_2])\nskills_12_dec_2_.replace(\"/n\", \" \")\n\nimport json\nwith open('/kaggle/working/skills_12_dec_3.jsonl', 'w') as f1:\n    json.dump(skills_12_dec_2, f1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T03:17:40.306345Z","iopub.execute_input":"2021-12-13T03:17:40.306662Z","iopub.status.idle":"2021-12-13T03:17:40.324643Z","shell.execute_reply.started":"2021-12-13T03:17:40.306622Z","shell.execute_reply":"2021-12-13T03:17:40.323895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Resume Extractor","metadata":{}},{"cell_type":"code","source":"path  =\"/kaggle/input/jobsextractor/sid.pdf\"\nspacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\nnlp, ruler = spacy_.nlp_model_initalization()\ntext_from_pdf = spacy_.pdf_to_text()\nclean_text = spacy_.cleaning_texts(text_from_pdf)\nget_skills_from_resume, others= spacy_.get_skills(nlp,clean_text)\n#pat = spacy_.get_create_patterns(patterns)\n#description = spacy_.get_job_discription(text_from_pdf, pat)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:55:05.400373Z","iopub.execute_input":"2021-12-11T07:55:05.400693Z","iopub.status.idle":"2021-12-11T07:55:08.371958Z","shell.execute_reply.started":"2021-12-11T07:55:05.400657Z","shell.execute_reply":"2021-12-11T07:55:08.371097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match = spacy_.get_matching_score(skills_required, get_skills_from_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:55:08.373844Z","iopub.execute_input":"2021-12-11T07:55:08.374145Z","iopub.status.idle":"2021-12-11T07:55:08.378474Z","shell.execute_reply.started":"2021-12-11T07:55:08.374102Z","shell.execute_reply":"2021-12-11T07:55:08.377809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_skills_from_text","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:46:27.079633Z","iopub.execute_input":"2021-12-10T08:46:27.080743Z","iopub.status.idle":"2021-12-10T08:46:27.087916Z","shell.execute_reply.started":"2021-12-10T08:46:27.08069Z","shell.execute_reply":"2021-12-10T08:46:27.08727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ruler = EntityRuler(nlp)\n#patterns = [{\"label\":\"ORG\",\"pattern\":\"Skills\"}]\n#ruler = nlp.add_pipe(\"entity_ruler\")\n#ruler.add_patterns(patterns)\n#ruler.patterns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy import displacy\n#sent = nlp(data[\"Resume_str\"].iloc[0])\n#displacy.render(sent, style=\"ent\", jupyter=True)\n\npatterns = data.Category.unique()\nfor a in patterns:\n    ruler.add_patterns([{\"label\": \"Job-Category\", \"pattern\": a}])\n    \noptions=[{\"ents\": \"Job-Category\", \"colors\": \"#ff3232\"},{\"ents\": \"SKILL\", \"colors\": \"#56c426\"}]\noptions=[{\"ents\": \"Job-Category\", \"colors\": \"#ff3232\"},{\"ents\": \"SKILL\", \"colors\": \"#56c426\"}]\ncolors = {\n    \"Job-Category\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n    \"SKILL\": \"linear-gradient(90deg, #9BE15D, #00E3AE)\",\n    \"ORG\": \"#ffd966\",\n    \"PERSON\": \"#e06666\",\n    \"GPE\": \"#9fc5e8\",\n    \"DATE\": \"#c27ba0\",\n    \"ORDINAL\": \"#674ea7\",\n    \"PRODUCT\": \"#f9cb9c\",\n}\noptions = {\n    \"ents\": [\n        \"Job-Category\",\n        \"SKILL\",\n        \"ORG\",\n        \"PERSON\",\n        \"GPE\",\n        \"DATE\",\n        \"ORDINAL\",\n        \"PRODUCT\",\n    ],\n    \"colors\": colors,\n}\n#sent = nlp(data[\"Resume_str\"].iloc[5])\n#displacy.render(sent, style=\"ent\", jupyter=True, options=options)\nsent2 = nlp(text_from_pdf)\ndisplacy.render(sent2, style=\"ent\", jupyter=True, options=options)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:38:55.085728Z","iopub.execute_input":"2021-12-09T06:38:55.086144Z","iopub.status.idle":"2021-12-09T06:38:55.487957Z","shell.execute_reply.started":"2021-12-09T06:38:55.086105Z","shell.execute_reply":"2021-12-09T06:38:55.486928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in sent2.sents:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T05:41:53.835308Z","iopub.execute_input":"2021-12-09T05:41:53.835652Z","iopub.status.idle":"2021-12-09T05:41:53.849546Z","shell.execute_reply.started":"2021-12-09T05:41:53.835619Z","shell.execute_reply":"2021-12-09T05:41:53.84862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in sent2.ents:\n    #print(i.label_)\n    if i.label_ == \"ORG\":\n        print(i.text)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:27:26.815328Z","iopub.execute_input":"2021-12-09T06:27:26.815789Z","iopub.status.idle":"2021-12-09T06:27:26.917352Z","shell.execute_reply.started":"2021-12-09T06:27:26.815642Z","shell.execute_reply":"2021-12-09T06:27:26.915848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning Text","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_data(resumeText):\n    resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n    resumeText = re.sub('#S+', '', resumeText)  # remove hashtags\n    #resumeText = re.sub('@S+', '  ', resumeText)  # remove mentions\n    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n    resumeText = re.sub(r'[^x00-x7f]',r' ', resumeText) \n    #resumeText = re.sub('s+', ' ', resumeText)  # remove extra whitespace\n    return resumeText\n\n#data['cleaned'] = data['Resume_str'].apply(lambda x : clean_data(x))\n\n\ndef clean_t(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [lemmer.lemmatize(word) for word in tokens if word not in stopwords]\n    text_final  = \" \".join(i for i in text)\n    return text_final\n\n\n#data['cleaned2'] = data['cleaned'].apply(lambda x :  clean_t(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:36:38.853643Z","iopub.execute_input":"2021-12-10T03:36:38.853944Z","iopub.status.idle":"2021-12-10T03:36:38.863612Z","shell.execute_reply.started":"2021-12-10T03:36:38.853914Z","shell.execute_reply":"2021-12-10T03:36:38.86238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = []\nfor i in words:\n    w.append(clean_data(i))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:36:39.40888Z","iopub.execute_input":"2021-12-10T03:36:39.409206Z","iopub.status.idle":"2021-12-10T03:36:39.843329Z","shell.execute_reply.started":"2021-12-10T03:36:39.409176Z","shell.execute_reply":"2021-12-10T03:36:39.842336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:36:39.965666Z","iopub.execute_input":"2021-12-10T03:36:39.966295Z","iopub.status.idle":"2021-12-10T03:36:39.991961Z","shell.execute_reply.started":"2021-12-10T03:36:39.966249Z","shell.execute_reply":"2021-12-10T03:36:39.991262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = \" \".join([i for i in words])\nw2 = clean_data(w).split(\" \")\nw2","metadata":{"execution":{"iopub.status.busy":"2021-12-10T02:46:20.130076Z","iopub.execute_input":"2021-12-10T02:46:20.130368Z","iopub.status.idle":"2021-12-10T02:46:20.231125Z","shell.execute_reply.started":"2021-12-10T02:46:20.130334Z","shell.execute_reply":"2021-12-10T02:46:20.230213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning Dataset","metadata":{}},{"cell_type":"code","source":"\nimport re\nclean = []\nfor i in range(data.shape[0]):\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        data[\"Resume_str\"].iloc[i],\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)\n    clean.append(review)\ndata['cleaned-1'] = clean","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:32.788149Z","iopub.execute_input":"2021-12-09T03:43:32.788524Z","iopub.status.idle":"2021-12-09T03:43:48.779096Z","shell.execute_reply.started":"2021-12-09T03:43:32.78849Z","shell.execute_reply":"2021-12-09T03:43:48.777812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning Selected file","metadata":{}},{"cell_type":"code","source":"def get_clean_file(text):\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        text,\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)\n    return review\n\nclean_text = get_clean_file(text_from_pdf)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:56:17.413685Z","iopub.execute_input":"2021-12-09T03:56:17.414018Z","iopub.status.idle":"2021-12-09T03:56:17.425249Z","shell.execute_reply.started":"2021-12-09T03:56:17.413984Z","shell.execute_reply":"2021-12-09T03:56:17.424478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_skills(text):\n    doc = nlp(text)\n    skills_ = []\n    for ent in doc.ents:\n        if \"SKILL\" in ent.label_ or ent.label_ == \"ORG\":\n            skills_.append(ent.text)\n    \n    return skills_\n\nget_skills_from_text = get_skills(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:56:18.489838Z","iopub.execute_input":"2021-12-09T03:56:18.490643Z","iopub.status.idle":"2021-12-09T03:56:18.63054Z","shell.execute_reply.started":"2021-12-09T03:56:18.490586Z","shell.execute_reply":"2021-12-09T03:56:18.629575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_skills_from_text ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:31:40.456653Z","iopub.execute_input":"2021-12-09T03:31:40.457218Z","iopub.status.idle":"2021-12-09T03:31:40.463309Z","shell.execute_reply.started":"2021-12-09T03:31:40.457177Z","shell.execute_reply":"2021-12-09T03:31:40.462648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Classification","metadata":{}},{"cell_type":"code","source":"unique_jobs = list(data['Category'].unique())\njobs = list(data['Category'])\nresume = list(data['cleaned-1'])\nlen(resume), len(jobs)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:48.781919Z","iopub.execute_input":"2021-12-09T03:43:48.78233Z","iopub.status.idle":"2021-12-09T03:43:48.792223Z","shell.execute_reply.started":"2021-12-09T03:43:48.78228Z","shell.execute_reply":"2021-12-09T03:43:48.79123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding text","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(resume+unique_jobs)\nvocab = len(tokenizer.word_index)+1\nprint(\"Vocab Szie\".format(vocab))\n\n##Fitting the tokenizer on resumes\ntokenizer_resume = tokenizer.texts_to_sequences(resume)\nlength_list = []\nfor tokenized_seq in tokenizer_resume:\n      length_list.append(len(tokenized_seq))\nmaxlen = np.array(length_list).max()\nprint(\"resume Max Length {}\".format(maxlen))\n\npad_resume = pad_sequences(tokenizer_resume, maxlen = maxlen, padding = 'post')\npad_resume.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:48.850659Z","iopub.execute_input":"2021-12-09T03:43:48.851917Z","iopub.status.idle":"2021-12-09T03:43:50.879906Z","shell.execute_reply.started":"2021-12-09T03:43:48.851851Z","shell.execute_reply":"2021-12-09T03:43:50.878975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding Categorical Labels","metadata":{}},{"cell_type":"code","source":"labelencoder = LabelEncoder()\ny = labelencoder.fit_transform(jobs)\ny_ = tf.keras.utils.to_categorical(y, 24)\ny_.shape\njobs2 = list(labelencoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:52.73952Z","iopub.execute_input":"2021-12-09T03:43:52.740531Z","iopub.status.idle":"2021-12-09T03:43:52.748922Z","shell.execute_reply.started":"2021-12-09T03:43:52.740468Z","shell.execute_reply":"2021-12-09T03:43:52.748159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Implmentation","metadata":{}},{"cell_type":"code","source":"## Train test split\nX_train, X_test, y_train, y_test = train_test_split(pad_resume, y_, test_size = 0.25)\nX_train.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:55.500416Z","iopub.execute_input":"2021-12-09T03:43:55.501409Z","iopub.status.idle":"2021-12-09T03:43:55.540487Z","shell.execute_reply.started":"2021-12-09T03:43:55.501329Z","shell.execute_reply":"2021-12-09T03:43:55.539463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating Glove 300d embedding","metadata":{}},{"cell_type":"code","source":"embeddings_index = dict()\nfile = open('/kaggle/input/jobsextractor/glove.6B.300d.txt')\nfor line in file:\n    values=  line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\nfile.close()\n\nembedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:44:13.863661Z","iopub.execute_input":"2021-12-09T00:44:13.864272Z","iopub.status.idle":"2021-12-09T00:45:05.533808Z","shell.execute_reply.started":"2021-12-09T00:44:13.864234Z","shell.execute_reply":"2021-12-09T00:45:05.533005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying Sequential Model keras","metadata":{}},{"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Embedding(vocab, 300, weights = [embedding_matrix], input_length=maxlen, trainable = False))\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(GlobalMaxPool1D())\nmodel1.add(Dense(32, activation='relu'))\nmodel1.add(Dense(24, activation='softmax'))\n\n# compile the model\nmodel1.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:47:38.89094Z","iopub.execute_input":"2021-12-09T00:47:38.89148Z","iopub.status.idle":"2021-12-09T00:47:39.045315Z","shell.execute_reply.started":"2021-12-09T00:47:38.891436Z","shell.execute_reply":"2021-12-09T00:47:39.044407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:47:40.107294Z","iopub.execute_input":"2021-12-09T00:47:40.107595Z","iopub.status.idle":"2021-12-09T00:47:40.116499Z","shell.execute_reply.started":"2021-12-09T00:47:40.107555Z","shell.execute_reply":"2021-12-09T00:47:40.115849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(X_train, y_train, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:47:43.640955Z","iopub.execute_input":"2021-12-09T00:47:43.641244Z","iopub.status.idle":"2021-12-09T01:00:53.374597Z","shell.execute_reply.started":"2021-12-09T00:47:43.641214Z","shell.execute_reply":"2021-12-09T01:00:53.373685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.save(\"/kaggle/working/job-categorical4.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:10:42.046061Z","iopub.execute_input":"2021-12-09T01:10:42.046554Z","iopub.status.idle":"2021-12-09T01:10:42.19047Z","shell.execute_reply.started":"2021-12-09T01:10:42.046505Z","shell.execute_reply":"2021-12-09T01:10:42.189578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodel1 = load_model(\"/kaggle/input/jobsextractor/job-categorical4.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:44:41.209706Z","iopub.execute_input":"2021-12-09T03:44:41.210034Z","iopub.status.idle":"2021-12-09T03:44:41.864856Z","shell.execute_reply.started":"2021-12-09T03:44:41.209996Z","shell.execute_reply":"2021-12-09T03:44:41.863625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\nprint(\"Prediction shape\".format(y_pred.shape))\n\ny_pred_l = np.where(y_pred == y_pred[0].max(),y_pred, int(0))\ny_final_pred_l = np.where(y_pred_l != y_pred[0].max(), y_pred_l,int(1))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:27:22.758664Z","iopub.execute_input":"2021-12-09T01:27:22.759312Z","iopub.status.idle":"2021-12-09T01:27:25.121763Z","shell.execute_reply.started":"2021-12-09T01:27:22.75927Z","shell.execute_reply":"2021-12-09T01:27:25.121134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def specific_prediction(clean_text, model, tokenizer):\n    ## Converting text to tokenized sequences\n    testing = tokenizer.texts_to_sequences([clean_text])\n    testing2 = []\n    for i in testing:\n        for j in i:\n            testing2.append(j)\n    # pading the sequences to equal length\n    testing_resume = pad_sequences([testing2], maxlen = maxlen, padding = 'post')\n    pred = model.predict([testing_resume])\n    \n    ## converting prediction to text again\n    y_pred_l = np.where(pred[0]>0.4,pred[0], int(0))\n    y_final_pred_l = np.where(y_pred_l<0.4, y_pred_l,int(1))\n    y_final_pred_l\n    index = []\n    for i in list(y_pred_l):\n        if i == 0:\n            continue\n        index.append(list(y_pred_l).index(i))\n        \n    index2 = []\n    for k in  sorted(list(pred[0]), reverse = True)[:5]:\n        #print(i)\n        j = list(pred[0]).index(k)\n        index2.append(j)\n    labels = []\n    for i in index2:\n        labels.append(jobs2[i])\n    \n    ## Getting final predictions\n    pred_label = list(y_final_pred_l).index(y_final_pred_l.max())\n    label = jobs2[pred_label]\n\n    return label, labels,index2\n\nlabel, labels, i = specific_prediction(clean_text, model1, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:08:45.913531Z","iopub.execute_input":"2021-12-09T04:08:45.914012Z","iopub.status.idle":"2021-12-09T04:08:45.997956Z","shell.execute_reply.started":"2021-12-09T04:08:45.91395Z","shell.execute_reply":"2021-12-09T04:08:45.996822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import classification_report\n#print(classification_report(y_test, y_final_pred_l))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:09:15.28235Z","iopub.execute_input":"2021-12-09T04:09:15.283275Z","iopub.status.idle":"2021-12-09T04:09:15.287879Z","shell.execute_reply.started":"2021-12-09T04:09:15.283217Z","shell.execute_reply":"2021-12-09T04:09:15.286793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bawa = \"\"\"   EDUCATION\nGeetanjali Institute Of Technical Studies.\nB.Tech. IN COMPUTER SCIENCE\nUdaipur, India | Expected May 2022\n\nCOMPUTER SKILLS\n• MS OFFICE • SQL • HTML5 \n• System administration• WordPress \n• Windows • LINUX/UNIX\n\nTECHNICAL SKILLS\n• Python • Tensorflow • C • C++ \n• Pytorch • Keras • Matlab•Git Data structures • AWS\n\nINTERPERSONAL SKILLS\n • Analytical Thinking• Problem Solving • Technical Writing •Public speaking • Team Leading\nCERTIFICATION COURSES\nCoursera:-\n Convolutional Neural Networks in TensorFlow \nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\nProject: Custom Prediction Routine on Google AI Platform\nPython data structures\nProgramming for Everybody (Getting Started with Python)\n  Linkedin :-\nAdvance Your Skills in Deep Learning and Neural Networks\nBuilding a Recommendation System with Python Machine Learning & AI\nBuilding Deep Learning Applications with Keras 2.0\n    Udemy:-\nComplete machine learning: from zero to mastery\n   Field of Interest\nNatural Language Processing\nCognitive science\nApplied computational science\nEEG \nDeep Neural Networks\nStatistical and Mathematical Computation of Machine Learning \nFeature Engineering\n  Hobbies\nCycling and Hiking\nReading Novels\nWeight Lifting\nVolleyball \n  Links\nGithub:https://github.com/hritvikgupta\nLinkedIn: https://linkedin.com/in/hritvik-gupta-8469611a3\nGoogle scholar:\nhttps://scholar.google.com/citations?user=ShxBp2MAAAAJ&hl=en\n\n\nPROFILE SUMMARY\nResearch enthusiastic with more than two year of experience in Natural language   processing and a year of working in Electroencephalogram (EEG) signal analysis. Extensively published in computing and AI  journals. I specifically work upon customized deep neural layers and optimization functions of neural layers. Also a confident speaker at conferences and has the ability to teach coursework and complex research to all kinds of people. \n\nEXPERIENCE\nIndian Institute Of Technology,Roorkee\n|RESEARCH INTERN\n|March 2021 - October 2021| Roorkee\nDeveloping customized models using Keras to classify the EEG signals by reaction time, go/no-go and passive tasks and Analyzing the EEG signals from young and old adults based on the rest and auditory cued reaction time tasks. \nApplying several Signal Pre-processing techniques like ICA, Signal-Space Projections and Source Estimation for removing the unwanted ECG and EOG artifacts, and PCA for dimensionality reduction. \nAcademics\nBachelors of Technology Computer science Engg.\n|Geetanjali Institute of Technical Studies, Udaipur, RJ\n|8.83 CGPA                                              July 2018 - July 2022  \nHigher Secondary School \n|CBSE- Delhi Public School, Udaipur, RJ \n|85.55 %                                                July 2017- July 2018  \n\nPROJECTS\nComprehensive Analysis of the Classification of Cognitive Load Of EEG Mental Load Signals\n|In Press Research Publication|March 2021 - present\nThe motive of this research is to classify between rest-active signals and the active part of the brain bearing a considerable high load on arithmetic tasks.\nAnalyzing the EEG signals from young and old adults based on the rest and arithmetic  cued response time tasks\nEntropy, Time Domain and Frequency Domain Feature analysis.\nAblation study using neural networks .\nOutcome Frontal Lobe is most active alongside parietal lobe. \nMulti Linguistic Text Generator\n|Final Year Project |september 2021 - present\nThe aim of Multi linguistic text generators is similar to that of the Google text generator and we worked  upon deep neural improvement\nTrained on less data but running on various algorithm to recast the encoder-decoder neural networks \nOutcome supposed to be Adaptive neural networks to Multi-linguistic text embedding. \n\nUnsupervised Text Summarizer Using LSA and Sentence based     topic modeling with BERT. \n|Research Paper IEEE Publication,Summer Internship Project| july 2020-oct   2020\nThe scope of this  research project which is based on Natural language processing to Summarize the long textual document to reduce database storage size and retain only relevant information\nUsed LSA topic modeling along with TFIDF keyword extractor for each sentence in a text document \nUsed BERT for text embeddings. Coalesce all embedding to be fed to neural architecture. \nObserved considerable decrease in size of data and increased in accuracy of the trained model as compared to that of previously published \n    Hybrid Text Summarization Using Elmo Embedding. \n    |Research Paper IEEE Publication,Winter  Project| Nov 2020- Feb 2021\nThis research project aims to build the algorithm to analyse unsupervised embedding when incorporate with supervised approach of ranking sentences\nText summarizer is built combining ELmo based text embedding which is unsupervised to the supervised approach of cosine similarity to build an efficient text summarizer. \nOutcome is a considerable increase in ROUGE-1 and ROUGE-L score is observed as compared to that of the previously published results on similar dataset. \n  Image Captioning\n  |Minor Project | June 2021            \nThis is one of the projects that I have built during the first 2 months of my internship at IIT while learning mathematical computation of keras neural networks. This includes creating an Image array using Res-Net model then building its own Custom Keras Lstm model for generating captions.\n\n     PUBLICATIONS \nH. Gupta and M. Patel, \"Method Of Text Summarization Using Lsa And Sentence Based Topic Modelling With Bert,\" 2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS), 2021, pp. 511-517, doi: 10.1109/ICAIS50930.2021.9395976.\n\nH. Gupta and M. Patel, \"Study of Extractive Text Summarizer Using The Elmo Embedding,\" 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2020, pp. 829-834, doi: 10.1109/I-SMAC49090.2020.924361\n\n       In Peer Publications\nAnalysing of EEG Signals Using RNN Classification\n|IEEE Scopus Journal |  November 25-27 2021   \n  \nComprehensive Analysis of the Classification of Cognitive Load Of EEG Mental Load Signals \n             |MDPI MOCAST |  January 15 -16 2022\n           \n     CONFERENCE PRESENTATIONS\n\n3rd International conference on innovations in power and advanced computing technologies I-PACT \n             | November 25-27 2021  \n\nInternational Conference on Artificial Intelligence and Smart Systems (ICAIS) \n | March 26-27 2021  \n\n4th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)\n             |November 25-27 2020  \n\n         LEADERSHIP AND VOLUNTEER EXPERIENCE\n\nHome Town Free Food Service \n|August 2020 - Present\n\nA part-time worker in an NGO which aims to provide free food to the Poor and necessities people in times of covid crisis. My job is to locate these people in places like bus stations, railway stations and certain sub-rural places.\n\nStudent Technical Club\n|September 2019 - Present\n\nLeader of the AI and AR/VR team in the technical club of Geetanjali institutes computer science which aims to enhance the technical skills of students in all fields and each respective field has a team and a team leader which specializes in that field.\n*\n\n\n\n\n\n\n\n\n\n\n\n\"\"\"\ndocter = \"\"\"PRAYANSH\nMAHESHWARI\nCONTACT DETAILS\nInfotainment Head Unit - User Engagement Dashboard\nThe aim of this project was to track user engagement in order to curate or\ncustomize the programs available and detect every deployment issue\nWorked on SQL(Hive) to extract and process client data and derived useful\ninsights to be included in Dashboard (Tableau)\nBuilt a dashboard which enabled Product, Marketing and Customer Experience\nTeam to take data-driven decision together\nCustomer Churn Management - Offer prioritization\nImplemented classification offer prioritization model to identify customer's\npropensity towards buying an offer and providing insights for different\nsegments of customers based on their behaviour\nThis resulted in a decrease in churn rate by ~2%\nIn addition, it helped the client realize that customers can be saved at a\nhigher-priced offer. $3M annualized revenue was realized from the project\nOrder Fulfillment - Request Management Portal\nThe aim of this project was to build a Report Management Tool which will\ndecrease the manual interventions by creating a web application tool\nThis resulted in saving 15 hrs. of work per week for 3 admin users. The tool is\nalso utilized by 2000+ users.\nMu Sigma\nTrainee Decision Scientist (June 2018 - May 2020)\nNirma University, ITNU\nB.Tech in Electronics and\nCommunication\nScored - 7.61 CGPA\nDAV School, Ajmer\nCBSE - Higher Secondary\nScored - 94%\nMayoor School, Ajmer\nCBSE - Senior Secondary\nScored - 9.8 CGPA\nEDUCATION\nSKILLS\nACHIVEMENTS\nSpot Award\nGreat job done at re-wiring the RMP\napplication components to make it\npresentable in a short turnaround time.\nHelping out your teammates around\nthe technical difficulties was\ncommendable.\nMentored a group of four inductees to guide them through problem space\njourney and provide actionable feedback for their skills enhancement\nVolunteered in M.A.D(Make a Difference) - Non-profit organization, working to\nensure better outcomes for children in orphanages and shelters across India\nOther interests - Cricket, Football, Adventure Sports and Painting\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:47:25.975753Z","iopub.execute_input":"2021-12-09T03:47:25.97608Z","iopub.status.idle":"2021-12-09T03:47:25.985796Z","shell.execute_reply.started":"2021-12-09T03:47:25.976046Z","shell.execute_reply":"2021-12-09T03:47:25.985024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    t = clean_t(docter)\n    t2 = clean_data(t)\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        docter,\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:34.601767Z","iopub.execute_input":"2021-12-09T03:52:34.602617Z","iopub.status.idle":"2021-12-09T03:52:34.617239Z","shell.execute_reply.started":"2021-12-09T03:52:34.602569Z","shell.execute_reply":"2021-12-09T03:52:34.616455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = nlp(review)\nfor i in e.ents:\n    print(i.text)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:38.138145Z","iopub.execute_input":"2021-12-09T03:52:38.139091Z","iopub.status.idle":"2021-12-09T03:52:38.205632Z","shell.execute_reply.started":"2021-12-09T03:52:38.139042Z","shell.execute_reply":"2021-12-09T03:52:38.204326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing = tokenizer.texts_to_sequences([t2])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:46.430959Z","iopub.execute_input":"2021-12-09T03:52:46.431897Z","iopub.status.idle":"2021-12-09T03:52:46.436199Z","shell.execute_reply.started":"2021-12-09T03:52:46.431842Z","shell.execute_reply":"2021-12-09T03:52:46.43549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:48.085317Z","iopub.execute_input":"2021-12-09T03:52:48.085643Z","iopub.status.idle":"2021-12-09T03:52:48.091169Z","shell.execute_reply.started":"2021-12-09T03:52:48.085612Z","shell.execute_reply":"2021-12-09T03:52:48.090112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing2 = []\nfor i in testing:\n    for j in i:\n        testing2.append(j)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:48.790234Z","iopub.execute_input":"2021-12-09T03:52:48.790604Z","iopub.status.idle":"2021-12-09T03:52:48.797514Z","shell.execute_reply.started":"2021-12-09T03:52:48.790566Z","shell.execute_reply":"2021-12-09T03:52:48.796484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing2","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:50.08355Z","iopub.execute_input":"2021-12-09T03:52:50.083857Z","iopub.status.idle":"2021-12-09T03:52:50.08863Z","shell.execute_reply.started":"2021-12-09T03:52:50.083826Z","shell.execute_reply":"2021-12-09T03:52:50.08772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_resume = pad_sequences([testing2], maxlen = maxlen, padding = 'post')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:50.590793Z","iopub.execute_input":"2021-12-09T03:52:50.591673Z","iopub.status.idle":"2021-12-09T03:52:50.596623Z","shell.execute_reply.started":"2021-12-09T03:52:50.591629Z","shell.execute_reply":"2021-12-09T03:52:50.595549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = model1.predict([testing_resume])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:51.94621Z","iopub.execute_input":"2021-12-09T03:52:51.946934Z","iopub.status.idle":"2021-12-09T03:52:52.027088Z","shell.execute_reply.started":"2021-12-09T03:52:51.946865Z","shell.execute_reply":"2021-12-09T03:52:52.025644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:53.862717Z","iopub.execute_input":"2021-12-09T03:52:53.863615Z","iopub.status.idle":"2021-12-09T03:52:53.870796Z","shell.execute_reply.started":"2021-12-09T03:52:53.86356Z","shell.execute_reply":"2021-12-09T03:52:53.869829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_resume.shape","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:52:54.410562Z","iopub.execute_input":"2021-12-09T03:52:54.411294Z","iopub.status.idle":"2021-12-09T03:52:54.418216Z","shell.execute_reply.started":"2021-12-09T03:52:54.41124Z","shell.execute_reply":"2021-12-09T03:52:54.417189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_l = np.where(p[0]>0.4,p[0], int(0))\ny_final_pred_l = np.where(y_pred_l<0.4, y_pred_l,int(1))\ny_final_pred_l\nindex = []\nfor i in list(y_pred_l):\n    if i == 0:\n        continue\n    index.append(list(y_pred_l).index(i))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:52:55.046223Z","iopub.execute_input":"2021-12-09T03:52:55.047107Z","iopub.status.idle":"2021-12-09T03:52:55.052828Z","shell.execute_reply.started":"2021-12-09T03:52:55.047048Z","shell.execute_reply":"2021-12-09T03:52:55.052145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(y_final_pred_l).index(y_final_pred_l.max())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:56.475718Z","iopub.execute_input":"2021-12-09T03:52:56.476436Z","iopub.status.idle":"2021-12-09T03:52:56.484267Z","shell.execute_reply.started":"2021-12-09T03:52:56.476365Z","shell.execute_reply":"2021-12-09T03:52:56.483196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in list(y_final_pred_l):\n    if i == 1:\n        print(list(y_final_pred_l).index(i))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:52:57.6726Z","iopub.execute_input":"2021-12-09T03:52:57.67292Z","iopub.status.idle":"2021-12-09T03:52:57.679688Z","shell.execute_reply.started":"2021-12-09T03:52:57.672889Z","shell.execute_reply":"2021-12-09T03:52:57.678763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_l","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:58.487947Z","iopub.execute_input":"2021-12-09T03:52:58.488442Z","iopub.status.idle":"2021-12-09T03:52:58.494686Z","shell.execute_reply.started":"2021-12-09T03:52:58.488403Z","shell.execute_reply":"2021-12-09T03:52:58.493831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(p[0]).index(p[0].max())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:48:05.427284Z","iopub.execute_input":"2021-12-09T03:48:05.427719Z","iopub.status.idle":"2021-12-09T03:48:05.43538Z","shell.execute_reply.started":"2021-12-09T03:48:05.427663Z","shell.execute_reply":"2021-12-09T03:48:05.434482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jobs2[5]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:48:09.180375Z","iopub.execute_input":"2021-12-09T03:48:09.180902Z","iopub.status.idle":"2021-12-09T03:48:09.186683Z","shell.execute_reply.started":"2021-12-09T03:48:09.180845Z","shell.execute_reply":"2021-12-09T03:48:09.185808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in index:\n    print(jobs2[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(data[data['Category'] == 'ENGINEERING']['Resume_str'])[117]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng = \"\"\"HARSHIT PALIWAL\nC O M P U T E R S C I E N C E S T U D E N T\nHello, my name is Harshit Paliwal. I’m\nstudying computer science engineering\nbecause I’m passionate about exploring ways\ntechnology can provide practical solutions to\neveryday problems.\nCONTACT DETAILS\nMobile: 8239251295\nEmail: harshitpaliwal95@gmail.com\nLinkedin: harshit-paliwal\nTwitter: harshit_hp\nGithub - harshitpaliwal95\nPortfolio - harshit-paliwal.netlify.app/\nEDUCATION\nB.tech\nGeetanjali Institute of Technical Studies,\nUdaipur\nCOMPUTER SCIENCE ENGINEERING\n6.2 CGPA | 2018 - present\n12th\nAlok Senior Secondary School, Udaipur\n51% | 2017 - 2018\nPROJECTS\nPortfolio site\n- build with Html Css & JavaScript || live\nDice game\n- build a dice game for 2 player logic build on Javascript\n- UI build on Html CSS code || live\nNaruto Game\n- build anime game that compares characters power\n-logic build on javaScript\n-UI build on Html Scss || live\n10th\nVidhya Bharti Senior Secondary School,\nUdaipur\n66% | 2015 - 2016\nSKILLS\nProgramming languages :\nc/c++, Java, JavaScript\nWeb Technologies:\nHTML5, CSS3, SCSS, BOOTSTRAP, REACTJS\nTools:\nGit, Github, Linux, Vs code, Command Line\nINTERESTS\nOpen Source Contribution\nLearn New Technology\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}