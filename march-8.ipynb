{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tika\nimport tensorflow\nimport pandas as pd\nimport keras\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom spacy.pipeline import EntityRuler \n#from wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nstopwords = nltk.corpus.stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nlemmer = WordNetLemmatizer()\nimport tensorflow as tf\n#from tensorflow import keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.layers import Conv1D, LSTM , Dense, BatchNormalization, Input, Bidirectional, Dropout\nfrom keras.models import Model\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n#import nolds\nimport scipy\n#import pyeeg\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport tensorflow\nfrom keras.layers import Lambda, Dot, Concatenate, Activation, Embedding, add, Conv1D,GlobalMaxPool1D\nfrom keras.models import Sequential\nimport pickle\nimport tempfile\nfrom scipy import signal\nfrom mne.time_frequency import psd_array_welch\n#from tf.keras.models import Sequential, load_model, save_model, Mode\n%matplotlib inline\nimport sklearn\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom gensim import corpora\nimport gensim\nimport re\n#spacy\nimport spacy\nfrom spacy.pipeline import EntityRuler\nfrom spacy.lang.en import English\nfrom spacy.tokens import Doc\nimport numpy as np\nfrom tika import parser","metadata":{"execution":{"iopub.status.busy":"2022-03-08T06:43:39.199346Z","iopub.execute_input":"2022-03-08T06:43:39.200074Z","iopub.status.idle":"2022-03-08T06:44:03.080877Z","shell.execute_reply.started":"2022-03-08T06:43:39.200005Z","shell.execute_reply":"2022-03-08T06:44:03.079979Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tika\n  Downloading tika-1.24.tar.gz (28 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tika) (59.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from tika) (2.25.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->tika) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->tika) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->tika) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->tika) (1.26.7)\nBuilding wheels for collected packages: tika\n  Building wheel for tika (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32891 sha256=7328d869bcb399691d90ed2b93c4f571c74770c4d4c808570bf823b95edf4e60\n  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\nSuccessfully built tika\nInstalling collected packages: tika\nSuccessfully installed tika-1.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pycaret","metadata":{"execution":{"iopub.status.busy":"2022-03-08T07:06:53.729033Z","iopub.execute_input":"2022-03-08T07:06:53.729274Z","iopub.status.idle":"2022-03-08T07:07:29.376681Z","shell.execute_reply.started":"2022-03-08T07:06:53.729241Z","shell.execute_reply":"2022-03-08T07:07:29.375153Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Collecting pycaret\n  Downloading pycaret-2.3.6-py3-none-any.whl (301 kB)\n     |████████████████████████████████| 301 kB 554 kB/s            \n\u001b[?25hRequirement already satisfied: textblob in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.17.1)\nCollecting pyod\n  Downloading pyod-0.9.8.tar.gz (114 kB)\n     |████████████████████████████████| 114 kB 8.2 MB/s            \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: lightgbm>=2.3.1 in /opt/conda/lib/python3.7/site-packages (from pycaret) (3.3.1)\nCollecting mlflow\n  Downloading mlflow-1.24.0-py3-none-any.whl (16.5 MB)\n     |████████████████████████████████| 16.5 MB 8.1 MB/s            \n\u001b[?25hRequirement already satisfied: pyLDAvis in /opt/conda/lib/python3.7/site-packages (from pycaret) (3.2.2)\nRequirement already satisfied: wordcloud in /opt/conda/lib/python3.7/site-packages (from pycaret) (1.8.1)\nRequirement already satisfied: pandas-profiling>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from pycaret) (3.0.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from pycaret) (3.2.4)\nCollecting gensim<4.0.0\n  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n     |████████████████████████████████| 24.2 MB 398 kB/s             \n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from pycaret) (1.3.4)\nCollecting spacy<2.4.0\n  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n     |████████████████████████████████| 10.4 MB 45.8 MB/s            \n\u001b[?25hRequirement already satisfied: ipywidgets in /opt/conda/lib/python3.7/site-packages (from pycaret) (7.6.5)\nRequirement already satisfied: seaborn in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.11.2)\nCollecting scipy<=1.5.4\n  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n     |████████████████████████████████| 25.9 MB 4.1 kB/s             \n\u001b[?25hCollecting pyyaml<6.0.0\n  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n     |████████████████████████████████| 636 kB 40.3 MB/s            \n\u001b[?25hRequirement already satisfied: scikit-plot in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.3.7)\nRequirement already satisfied: mlxtend>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.19.0)\nRequirement already satisfied: yellowbrick>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from pycaret) (1.3.post1)\nCollecting imbalanced-learn==0.7.0\n  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n     |████████████████████████████████| 167 kB 41.1 MB/s            \n\u001b[?25hRequirement already satisfied: scikit-learn==0.23.2 in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.23.2)\nRequirement already satisfied: Boruta in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from pycaret) (3.5.0)\nRequirement already satisfied: kmodes>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.11.1)\nRequirement already satisfied: cufflinks>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.17.3)\nRequirement already satisfied: plotly>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from pycaret) (5.4.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from pycaret) (1.1.0)\nRequirement already satisfied: IPython in /opt/conda/lib/python3.7/site-packages (from pycaret) (7.29.0)\nRequirement already satisfied: umap-learn in /opt/conda/lib/python3.7/site-packages (from pycaret) (0.5.2)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0->pycaret) (1.19.5)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2->pycaret) (3.0.0)\nRequirement already satisfied: colorlover>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from cufflinks>=0.17.0->pycaret) (0.3.0)\nRequirement already satisfied: setuptools>=34.4.1 in /opt/conda/lib/python3.7/site-packages (from cufflinks>=0.17.0->pycaret) (59.1.1)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from cufflinks>=0.17.0->pycaret) (1.16.0)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0.0->pycaret) (5.2.1)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (2.10.0)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (0.1.3)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (5.1.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (3.0.22)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (4.8.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (0.2.0)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (0.18.1)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from IPython->pycaret) (5.1.0)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret) (1.0.2)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret) (0.2.0)\nRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret) (6.5.0)\nRequirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret) (5.1.3)\nRequirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets->pycaret) (3.5.2)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from lightgbm>=2.3.1->pycaret) (0.37.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (21.0)\nRequirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (6.3.2)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (8.2.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (4.28.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (2.8.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pycaret) (3.0.6)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->pycaret) (2021.3)\nRequirement already satisfied: tangled-up-in-unicode==0.1.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (0.1.0)\nRequirement already satisfied: requests>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (2.25.1)\nRequirement already satisfied: jinja2>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (3.0.3)\nRequirement already satisfied: missingno>=0.4.2 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (0.4.2)\nRequirement already satisfied: pydantic>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (1.8.2)\nRequirement already satisfied: htmlmin>=0.1.12 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (0.1.12)\nRequirement already satisfied: phik>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (0.12.0)\nRequirement already satisfied: tqdm>=4.48.2 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (4.62.3)\nRequirement already satisfied: visions[type_image_path]==0.7.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling>=2.8.0->pycaret) (0.7.1)\nRequirement already satisfied: bottleneck in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret) (1.3.2)\nRequirement already satisfied: networkx>=2.4 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret) (2.6.3)\nRequirement already satisfied: multimethod==1.4 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret) (1.4)\nRequirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret) (21.2.0)\nRequirement already satisfied: imagehash in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret) (4.2.1)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.4.1->pycaret) (8.0.1)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret) (3.0.6)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret) (0.8.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret) (1.0.6)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret) (2.0.6)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0->pycaret) (0.7.5)\nCollecting thinc<7.5.0,>=7.4.1\n  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n     |████████████████████████████████| 1.0 MB 35.8 MB/s            \n\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\nCollecting catalogue<1.1.0,>=0.0.7\n  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\nCollecting srsly<1.1.0,>=1.0.2\n  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n     |████████████████████████████████| 184 kB 50.7 MB/s            \n\u001b[?25hCollecting gunicorn\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n     |████████████████████████████████| 79 kB 5.1 MB/s             \n\u001b[?25hRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (1.7.5)\nRequirement already satisfied: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (2.0.2)\nRequirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (4.8.2)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (2.0.0)\nRequirement already satisfied: protobuf>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (3.19.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (8.0.3)\nRequirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (3.1.24)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (0.3)\nRequirement already satisfied: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (5.0.3)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: sqlparse>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (0.4.2)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.18.7-py3-none-any.whl (17 kB)\nRequirement already satisfied: sqlalchemy in /opt/conda/lib/python3.7/site-packages (from mlflow->pycaret) (1.4.27)\nCollecting databricks-cli>=0.8.7\n  Downloading databricks-cli-0.16.4.tar.gz (58 kB)\n     |████████████████████████████████| 58 kB 3.3 MB/s             \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret) (0.18.2)\nRequirement already satisfied: funcy in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret) (1.16)\nRequirement already satisfied: numexpr in /opt/conda/lib/python3.7/site-packages (from pyLDAvis->pycaret) (2.7.3)\nRequirement already satisfied: numba>=0.35 in /opt/conda/lib/python3.7/site-packages (from pyod->pycaret) (0.54.1)\nRequirement already satisfied: statsmodels in /opt/conda/lib/python3.7/site-packages (from pyod->pycaret) (0.12.2)\nRequirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.7/site-packages (from umap-learn->pycaret) (0.5.5)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow->pycaret) (0.8.9)\nRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow->pycaret) (1.2.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow->pycaret) (4.0.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow->pycaret) (3.10.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow->pycaret) (3.6.0)\nRequirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->pycaret) (6.1)\nRequirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->pycaret) (1.5.1)\nRequirement already satisfied: argcomplete>=1.12.3 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->pycaret) (1.12.3)\nRequirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets->pycaret) (7.0.6)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->IPython->pycaret) (0.8.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.11.1->pandas-profiling>=2.8.0->pycaret) (2.0.1)\nRequirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->pycaret) (4.9.1)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets->pycaret) (3.2.0)\nRequirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in /opt/conda/lib/python3.7/site-packages (from numba>=0.35->pyod->pycaret) (0.37.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->IPython->pycaret) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->pycaret) (0.2.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (2021.10.8)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.24.0->pandas-profiling>=2.8.0->pycaret) (1.26.7)\nRequirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->pycaret) (1.2.2)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets->pycaret) (6.4.6)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow->pycaret) (5.4.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow->pycaret) (1.1.6)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy->mlflow->pycaret) (1.1.2)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow->pycaret) (2.0.1)\nRequirement already satisfied: Werkzeug>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow->pycaret) (2.0.2)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow->pycaret) (0.12.0)\nRequirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.7/site-packages (from statsmodels->pyod->pycaret) (0.5.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow->pycaret) (3.0.5)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->pycaret) (0.18.0)\nRequirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->pycaret) (1.5.1)\nRequirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets->pycaret) (22.3.0)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (21.1.0)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (1.8.0)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.12.1)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (6.3.0)\nRequirement already satisfied: PyWavelets in /opt/conda/lib/python3.7/site-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling>=2.8.0->pycaret) (1.2.0)\nRequirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (1.15.0)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.8.4)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (4.1.0)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.5.8)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.1.2)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.5.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.7.1)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (1.5.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (2.21)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets->pycaret) (0.5.1)\nBuilding wheels for collected packages: pyod, databricks-cli\n  Building wheel for pyod (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyod: filename=pyod-0.9.8-py3-none-any.whl size=136773 sha256=d65540fe156f3b6f14374242018d0e0e4292da6b09f27c2f535b8d5c023a1b99\n  Stored in directory: /root/.cache/pip/wheels/ba/8f/95/6cb376aec9fae09d9b1622d1662c902b522deb353cb80836a6\n  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.16.4-py3-none-any.whl size=106877 sha256=46637669b472a41855eb891d008894c7d6c800e7a94dc7b7e987d67241372b43\n  Stored in directory: /root/.cache/pip/wheels/a2/a1/6d/fa1d22ea25ed8593887437fe1c7e00f6ef307fc240ccd4dc5c\nSuccessfully built pyod databricks-cli\nInstalling collected packages: scipy, srsly, plac, catalogue, thinc, querystring-parser, pyyaml, prometheus-flask-exporter, gunicorn, databricks-cli, spacy, pyod, mlflow, imbalanced-learn, gensim, pycaret\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.7.2\n    Uninstalling scipy-1.7.2:\n      Successfully uninstalled scipy-1.7.2\n  Attempting uninstall: srsly\n    Found existing installation: srsly 2.4.2\n    Uninstalling srsly-2.4.2:\n      Successfully uninstalled srsly-2.4.2\n  Attempting uninstall: catalogue\n    Found existing installation: catalogue 2.0.6\n    Uninstalling catalogue-2.0.6:\n      Successfully uninstalled catalogue-2.0.6\n  Attempting uninstall: thinc\n    Found existing installation: thinc 8.0.13\n    Uninstalling thinc-8.0.13:\n      Successfully uninstalled thinc-8.0.13\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0\n    Uninstalling PyYAML-6.0:\n      Successfully uninstalled PyYAML-6.0\n  Attempting uninstall: spacy\n    Found existing installation: spacy 3.1.4\n    Uninstalling spacy-3.1.4:\n      Successfully uninstalled spacy-3.1.4\n  Attempting uninstall: imbalanced-learn\n    Found existing installation: imbalanced-learn 0.8.1\n    Uninstalling imbalanced-learn-0.8.1:\n      Successfully uninstalled imbalanced-learn-0.8.1\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.0.1\n    Uninstalling gensim-4.0.1:\n      Successfully uninstalled gensim-4.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscattertext 0.1.5 requires gensim>=4.0.0, but you have gensim 3.8.3 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.0 which is incompatible.\nen-core-web-sm 3.1.0 requires spacy<3.2.0,>=3.1.0, but you have spacy 2.3.7 which is incompatible.\nen-core-web-lg 3.1.0 requires spacy<3.2.0,>=3.1.0, but you have spacy 2.3.7 which is incompatible.\u001b[0m\nSuccessfully installed catalogue-1.0.0 databricks-cli-0.16.4 gensim-3.8.3 gunicorn-20.1.0 imbalanced-learn-0.7.0 mlflow-1.24.0 plac-1.1.3 prometheus-flask-exporter-0.18.7 pycaret-2.3.6 pyod-0.9.8 pyyaml-5.4.1 querystring-parser-1.2.4 scipy-1.5.4 spacy-2.3.7 srsly-1.0.5 thinc-7.4.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Deploying flask application","metadata":{}},{"cell_type":"code","source":"from flask import Flask,request, url_for, redirect, render_template, jsonify\nfrom pycaret.regression import *\nimport pandas as pd\nimport pickle\nimport numpy as np\n# Initalise the Flask app\napp = Flask(__name__)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T07:07:29.379545Z","iopub.execute_input":"2022-03-08T07:07:29.379849Z","iopub.status.idle":"2022-03-08T07:07:31.110228Z","shell.execute_reply.started":"2022-03-08T07:07:29.379814Z","shell.execute_reply":"2022-03-08T07:07:31.108986Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"cols = ['job', 'skills_required', 'salary', 'job_description', 'salary']\n\n@app.route('/')\ndef home():\n    return render_template(\"home.html\")\n\n@app.route('/predict',methods=['POST'])\ndef predict():\n    int_features = [x for x in request.form.values()]\n    final = np.array(int_features)\n    data_unseen = pd.DataFrame([final], columns = cols)\n    prediction = predict_model(model, data=data_unseen, round = 0)\n    prediction = int(prediction.Label[0])\n    return render_template('home.html',pred='Expected Bill will be {}'.format(prediction))\n\n\n\nif __name__ == '__main__':\n    app.run(debug=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### dataset1","metadata":{}},{"cell_type":"code","source":"\ndata = pd.read_csv(\"/kaggle/input/jobsextractor/Resume.csv\")\\\n#data['Category'].value_counts()\nimport re\nclean = []\nfor i in range(data.shape[0]):\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        data[\"Resume_str\"].iloc[i],\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)\n    clean.append(review)\ndata['cleaned-1'] = clean","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:02:43.234766Z","iopub.execute_input":"2022-03-08T04:02:43.235443Z","iopub.status.idle":"2022-03-08T04:03:06.457945Z","shell.execute_reply.started":"2022-03-08T04:02:43.235403Z","shell.execute_reply":"2022-03-08T04:03:06.456767Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#data = data.reindex(np.random.permutation(data.index))","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:03:06.459895Z","iopub.execute_input":"2022-03-08T04:03:06.460633Z","iopub.status.idle":"2022-03-08T04:03:06.465186Z","shell.execute_reply.started":"2022-03-08T04:03:06.460586Z","shell.execute_reply":"2022-03-08T04:03:06.463921Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### dataset2","metadata":{}},{"cell_type":"code","source":"import csv\ntsv_file = open(\"/kaggle/input/jobsextractor/train.tsv\")\nread_tsv = csv.reader(tsv_file, delimiter=\"\\t\")\njobs_description = []\nfor row in read_tsv:\n      jobs_description.append(row)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:03:06.466641Z","iopub.execute_input":"2022-03-08T04:03:06.467173Z","iopub.status.idle":"2022-03-08T04:03:06.793526Z","shell.execute_reply.started":"2022-03-08T04:03:06.467131Z","shell.execute_reply":"2022-03-08T04:03:06.792422Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"##### dataset3","metadata":{}},{"cell_type":"code","source":"jobs_description2 = pd.read_csv(\"/kaggle/input/jobsextractor/dice_com-job_us_sample.csv\")\ndescriptions_jobs = jobs_description2['jobdescription']\npath  = \"/kaggle/input/jobsextractor/yashi.pdf\"\nspacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\nclean_descriptions = []\nfor i in descriptions_jobs:\n    clean_descriptions.append(spacy_.cleaning_texts(i))\n    \n    docs = clean_descriptions[:1000]\ndictionary = corpora.Dictionary(d.split() for d in docs)\nbow = [dictionary.doc2bow(d.split()) for d in docs]\nlda = gensim.models.ldamodel.LdaModel\nnum_topics = 10\nldamodel = lda(\n    bow, \n    num_topics=num_topics, \n    id2word=dictionary, \n    passes=50, \n    minimum_probability=0\n)\n#ldamodel.print_topics(num_topics=num_topics)\n\ncommon_words = []\nfor index, topic in ldamodel.show_topics(formatted=False, num_words= 30):\n        for w in topic:\n            common_words.append(w[0])\ncommon_word = \" \".join([i for i in common_words])\nwith open(\"/kaggle/working/lda_words.txt\", 'w') as f:\n    f.write(common_word)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:03:06.796068Z","iopub.execute_input":"2022-03-08T04:03:06.796471Z","iopub.status.idle":"2022-03-08T04:03:08.345740Z","shell.execute_reply.started":"2022-03-08T04:03:06.796425Z","shell.execute_reply":"2022-03-08T04:03:08.345039Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/input/jobsextractor/j.txt\")\ntxt = f.read()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:16:31.702119Z","iopub.execute_input":"2021-12-10T08:16:31.702548Z","iopub.status.idle":"2021-12-10T08:16:31.709893Z","shell.execute_reply.started":"2021-12-10T08:16:31.702509Z","shell.execute_reply":"2021-12-10T08:16:31.708889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/input/jobsextractor/j.txt\")\ntxt = f.read()\n\nwords_list2 = []\nc = txt.split(\"-\")\nfor i in c:\n    t = i.split('\\n')\n    if len(t) == 2:\n        words_list2.append(t[1])\n\nfinal_words_list2 = []\nfor i in words_list2:\n    #print(i)\n    final_words_list2.append(spacy_.cleaning_texts(i.split(\".\")[1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:25:03.739578Z","iopub.execute_input":"2021-12-11T03:25:03.741233Z","iopub.status.idle":"2021-12-11T03:25:03.76721Z","shell.execute_reply.started":"2021-12-11T03:25:03.741158Z","shell.execute_reply":"2021-12-11T03:25:03.766399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r'\\b(?:total|staff)\\b'\npatterns3= []\nfor i in final_words_list2:\n    patterns3.append(r'\\b(?:'+str(i)+')\\b')","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:30:06.837396Z","iopub.execute_input":"2021-12-11T03:30:06.83777Z","iopub.status.idle":"2021-12-11T03:30:06.844884Z","shell.execute_reply.started":"2021-12-11T03:30:06.837729Z","shell.execute_reply":"2021-12-11T03:30:06.843473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final_words_list2\n#patterns3","metadata":{"execution":{"iopub.status.busy":"2021-12-11T03:59:40.239745Z","iopub.execute_input":"2021-12-11T03:59:40.240336Z","iopub.status.idle":"2021-12-11T03:59:40.245995Z","shell.execute_reply.started":"2021-12-11T03:59:40.240083Z","shell.execute_reply":"2021-12-11T03:59:40.244964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### dataset4\n","metadata":{}},{"cell_type":"code","source":"data_science_dataset = pd.read_csv(\"/kaggle/input/jobsextractor/DATA_SCIENCE.csv\")\n\nspacy_1 =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\ndocs = data_science_dataset['job_description'][:1000]\ntexts = []\nfor i in docs:\n    texts.append(spacy_1.cleaning_texts(i))\ndictionary = corpora.Dictionary(d.split() for d in texts)\nbow = [dictionary.doc2bow(d.split()) for d in texts]\nlda = gensim.models.ldamodel.LdaModel\nnum_topics = 30\nldamodel = lda(\n    bow, \n    num_topics=num_topics, \n    id2word=dictionary, \n    passes=50, \n    minimum_probability=0\n)\n\n#ldamodel.print_topics(num_topics=num_topics)\ncommon_words2 = []\nfor index, topic in ldamodel.show_topics(formatted=False, num_words= 100):\n        for w in topic:\n            common_words2.append(w[0])\n            \n            \n# from spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = English()\nfs = []\nfor i in range(len(descriptions_jobs )):\n    my_doc = nlp(descriptions_jobs[i])\n\n    token_list = []\n    for token in my_doc:\n        token_list.append(token.text)\n\n    filtered_sentence =[] \n\n    for word in token_list:\n        lexeme = nlp.vocab[word]\n        if lexeme.is_stop == False:\n            filtered_sentence.append(word) \n    for i in filtered_sentence:\n        fs.append(i)\n    #print(token_list)\n    #print(filtered_sentence) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:03:10.423198Z","iopub.execute_input":"2022-03-08T04:03:10.424402Z","iopub.status.idle":"2022-03-08T04:03:11.621025Z","shell.execute_reply.started":"2022-03-08T04:03:10.424343Z","shell.execute_reply":"2022-03-08T04:03:11.619955Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#t = jobs_description[11][1]\n#def removearticles(text):\n#    t2 = re.sub('\\s+(a|an|and|the)(\\s+)', '\\2', text)\n#    print(t2)\n    \n#removearticles(t)\n#c = nlp(descriptions_jobs[0])\n#for i in c.ents:\n #   print(i.text)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:44:42.672668Z","iopub.execute_input":"2021-12-10T08:44:42.672981Z","iopub.status.idle":"2021-12-10T08:44:42.678137Z","shell.execute_reply.started":"2021-12-10T08:44:42.672948Z","shell.execute_reply":"2021-12-10T08:44:42.677385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Implementing pipeline","metadata":{}},{"cell_type":"code","source":"class resume_spacy_pdf_clean_skills():\n    \n    def __init__(self, path_to_pdf, cleaning_type):\n        self.path = path_to_pdf\n        #clean_types = [\"mycleaning\", \"specific_cleaning\"]\n        self.cleaning_type = cleaning_type\n        \n        \n    def nlp_model_initalization(self):\n        nlp = spacy.load(\"en_core_web_lg\")\n        ruler = nlp.add_pipe(\"entity_ruler\")\n        ruler.from_disk(\"/kaggle/input/jobsextractor/jobs2.jsonl\")\n        #ruler.from_disk(\"/kaggle/input/jobsextractor/skills_12_dec_2.json\")\n        return nlp, ruler\n    \n    def pdf_to_text(self):\n        raw = parser.from_file(self.path)\n        text = raw['content']\n        return text\n    \n    def cleaning_texts(self, text):\n        if self.cleaning_type == \"my_cleaning\":\n            resumeText = text\n            resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n            resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n            resumeText = re.sub('#S+', '', resumeText)  # remove hashtags\n            resumeText = re.sub('@S+', '  ', resumeText)  # remove mentions\n            resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n            resumeText = re.sub(r'[^x00-x7f]',r' ', resumeText) \n            resumeText = re.sub('s+', ' ', resumeText)  # remove extra whitespace\n            text = resumeText\n            text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n            tokens = re.split('\\W+', text)\n            text = [lemmer.lemmatize(word) for word in tokens if word not in stopwords]\n            review  = \" \".join(i for i in text)\n        \n        if self.cleaning_type == 'specific_cleaning':\n\n                review = re.sub(\n                    '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n                    \" \",\n                    text,\n                )\n                review = review.lower()\n                review = review.split()\n                lm = WordNetLemmatizer()\n                review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n                review = \" \".join(review)\n        return review\n\n    def get_skills(self, nlp, text):\n        doc = nlp(text)\n        skills_ = []\n        others = []\n        for ent in doc.ents:\n            if \"SKILL\" in ent.label_:\n                skills_.append(ent.text)\n            elif ent.label_ == \"ORG\":\n                others.append(ent.text)\n         \n        return skills_, others\n    \n    def get_job_resume_discription(self,text, pattern):\n        ans = []\n        pattern2 = pattern\n        sp = text.split(\"\\n\")\n        if len(sp) <=3 : \n            text2 = text.split(\"\\xa0\")\n            for pat in pattern2:\n                for t in text2:\n                     if re.search(pat, t) != None:\n                            ans.append(t)\n        else:\n            for pat in pattern2:\n                for t in text.split('\\n'):\n                     if re.search(pat, t) != None:\n                            ans.append(t)\n\n        ans2 = \" \".join([i for i in list(set(ans))])\n        #final  = clean_data(ans2)\n        return ans2 , list(set(ans))\n    \n    def get_description_without_re(self, text, pattern):\n        sent = []\n        text2 = inp.split('\\n')\n        #p5_ = p5.split(\" \")+p6.split(\" \")\n        for i in p5.split(\" \"):\n            for j in text2:\n                if i.lower() in j.lower() and i not in sent:\n                    sent.append(j)\n                    #print(\"Text:-{}, pat:-{}\".format(j,i))\n       \n        \n        return sent\n                \n\n    def get_create_patterns(self, text):\n        \n        pattern2 = [r'\\b(?i)'+'plan'+r'\\b', r'\\b(?i)'+'years'+r'\\b',\n        r'\\b(?i)'+'experience'+r'\\b',\n        r'\\b(?i)'+'worked'+r'\\b',\n        r'\\b(?i)'+'willing'+r'\\b',\n        r'\\b(?i)'+'knowledge'+r'\\b',\n        r'\\b(?i)'+'interview'+r'\\b', \n        r'\\b(?i)'+'applicants'+r'\\b',\n        r'\\b(?i)'+'interview'+r'\\b',\n        r'\\b(?i)'+'immediate'+r'\\b',\n        r'\\b(?i)'+'interested'+r'\\b',\n        r'\\b(?i)'+'opening'+r'\\b',]\n\n        #for i in w:\n         #           pattern2.append(r'\\b(?i)'+ str(i) + r'\\b')\n        w2 = text.split(\" \")\n        for i in w2:\n            pattern2.append(r'\\b(?i)'+str(i)+r'\\b')\n    \n        return pattern2\n    \n    def get_description_skill(self,nlp,des):\n        skill = []\n        des = des.lower()\n        d = nlp(des)\n        for i in d.ents:\n            if 'SKILL' in i.label_:\n                skill.append(i.text)\n        return set(skill)\n    \n    def get_salary(self, text_from_pdf):\n        pat2 = [r'\\b(?i)'+'salary'+r'\\b', r'\\b(?i)'+'Rs'+r'\\b', r'\\b(?i)'+'rs'+r'\\b']\n\n        sal = []\n        for p in pat2:\n            for i in text_from_pdf.lower().split(\"\\n\"):\n                if re.search(p, i)!= None:\n                    sal.append(i)\n        return sal\n    \n    \n    \n    #b =\"posting for a computer engineer job in microsoft\"\n    #v = b.index(\"engineer\")\n    def get_job_from_training_spacy_model(self, data,nlp, clean_data):\n    ## Making data for training\n        TRAIN_DATA = data\n\n\n        ##Loading model from NLP\n\n        LABEL = \"JOB\"\n        nlp, ruler = spacy_.nlp_model_initalization()\n        pipes1 = nlp.pipe_names\n        ner=nlp.get_pipe(\"ner\")\n        optimizer = nlp.resume_training()\n        move_names = list(ner.move_names)\n        #pipe_exceptions = pipes1\n        pipe_exceptions = [\"ner\", \"tagger\", \"tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        #pipe_exceptions = [\"ner\", \"tagger\", \"tok2vec\"]\n        \n        for _, annotations in TRAIN_DATA:\n            for ent in annotations.get(\"entities\"):\n                ner.add_label(ent[2])\n\n\n\n        ### Training the model\n        # Import requirements\n        import random\n        from spacy.util import minibatch, compounding\n        from pathlib import Path\n        from spacy.training.example import Example\n\n        # TRAINING THE MODEL\n        with nlp.disable_pipes(*other_pipes):\n\n          # Training for 30 iterations\n          for iteration in range(30):\n\n            # shuufling examples  before every iteration\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            #annotations = [entities for text, entities in batches]\n            for batch in batches:\n                    texts, annotations = zip(*batch)\n\n                    example = []\n                    # Update the model with iterating each text\n                    for i in range(len(texts)):\n                        doc = nlp.make_doc(texts[i])\n                        example.append(Example.from_dict(doc, annotations[i]))\n\n                    # Update the model\n                    nlp.update(example, drop=0.5, losses=losses)\n\n\n        ### Saving the model\n        from pathlib import Path\n        output_dir=Path('/kaggle/working/model')\n\n        # Saving the model to the output directory\n        if not output_dir.exists():\n              output_dir.mkdir()\n        nlp.meta['name'] = 'my_ner'  # rename model\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)\n\n        job = []\n        text = nlp(clean_text.lower())\n        for i in text.ents:\n            if \"JOB\" in i.label_:\n                job.append(i.text)\n\n        return job\n    \n    def get_number_of_post(self, text_from_pdf):\n        post = []\n        pat2 = [r'\\b(?i)'+'senior'+r'\\b',r'\\b(?i)'+'Trainee'+r'\\b',r'\\b(?i)'+'post'+r'\\b',r'\\b(?i)'+'reserch fellow'+r'\\b',r'\\b(?i)'+'junior'+r'\\b',r'\\b(?i)'+'nos'+r'\\b', r'\\b(?i)'+'position'+r'\\b', r'\\b(?i)'+'required'+r'\\b', r'\\b(?i)'+'posting'+r'\\b', r'\\b(?i)'+'vocation'+r'\\b',  r'\\b(?i)'+'vacancy'+r'\\b',  r'\\b(?i)'+'opening'+r'\\b',  r'\\b(?i)'+'place'+r'\\b']\n        sal = []\n        for p in pat2:\n            for i in text_from_pdf.lower().split(\"\\n\"):\n                if re.search(p, i)!= None:\n                    post.append(i)\n\n        return post\n\n                \n    def get_matching_score(self,req, original):\n        req_skills = req\n        resume_skills = original\n        score = 0\n        for x in req_skills:\n            if x in resume_skills:\n                score += 1\n        req_skills_len = len(req_skills)\n        match = round(score / req_skills_len * 100, 1)\n\n        print(f\"The current Resume is {match}% matched to your requirements\")\n        return match \n\n        \n\n        \ndef train_nlp_model_entity(model, data):\n    nlp = model\n    ner = nlp.get_pipe(\"ner\")\n    for _, annotations in data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    pipe_exceptions = ['ner', \"trf_wordpiecer\", \"trf_tok2vec\"]\n    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n    from spacy.training.example import Example\n    import random\n    from spacy.util import minibatch, compounding\n    from pathlib import Path\n\n    # TRAINING THE MODEL\n    with nlp.disable_pipes(*unaffected_pipes):\n        for iteration in range(30):\n\n            # shuufling examples  before every iteration\n            random.shuffle(data)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(data, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                for texts, annotations in batch:\n                    doc = nlp.make_doc(texts)\n                    example = Example.from_dict(doc , annotations)\n                    nlp.update( [example], # batch of annotations\n                                drop=0.5,  # dropout - make it harder to memorise data\n                                losses=losses,\n\n                            )\n                    print(\"Losses\", losses)\n    return nlp\n#saving the new spacy model zip file and loading \nimport shutil\ndef save_model_spacy(path, output_file_name):\n    output_dir = Path(path)\n    nlp.to_disk(output_dir)\n    shutil.make_archive(output_file_name, 'zip', output_dir)\n    \ndef load_updated_model(output_dir, text):\n    nlp_updated = spacy.load(output_dir)\n    doc1 = nlp_updated(text)\n    return doc1\n\ndef trained_spacy_model_jobs(nlp, data):\n    nlp_updated = train_nlp_model_entity(nlp, data)\n    doc = nlp_updated(text_from_pdf)\n    jobs = [ent.text for ent in doc.ents]\n    \n    return jobs","metadata":{"execution":{"iopub.status.busy":"2022-03-08T07:19:12.871241Z","iopub.execute_input":"2022-03-08T07:19:12.871677Z","iopub.status.idle":"2022-03-08T07:19:12.932259Z","shell.execute_reply.started":"2022-03-08T07:19:12.871646Z","shell.execute_reply":"2022-03-08T07:19:12.931416Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\np = \"LEADERSHIP Accomplished Achieved Administered Analyzed Assigned Attained Chaired Consolidated Contracted Coordinated Delegated Developed Directed Earned Evaluated Executed Handled Headed Impacted Improved Increased Led Mastered Orchestrated Organized Oversaw Planned Predicted Prioritized Produced Proved Recommended  Regulated Reorganized Reviewed Scheduled Spearheaded Strengthened Supervised Surpassed  COMMUNICATION  Addressed Arbitrated Arranged Authored Collaborated Convinced Corresponded Delivered Developed Directed Documented Drafted Edited Energized Enlisted Formulated Influenced Interpreted Lectured Liaised Mediated Moderated Negotiated Persuaded Presented Promoted Publicized Reconciled Recruited Reported Rewrote Spoke Suggested Synthesized Translated Verbalized Wrote RESEARCH Clarified Collected Concluded Conducted Constructed Critiqued Derived Determined Diagnosed Discovered Evaluated Examined Extracted Formed Identified Inspected Interpreted Interviewed Investigated Modeled Organized Resolved Reviewed Summarized Surveyed Systematized Tested TECHNICAL Assembled Built Calculated Computed Designed Devised Engineered Fabricated Installed Maintained Operated Optimized Overhauled Programmed Remodeled Repaired Solved Standardized Streamlined Upgraded TEACHING Adapted Advised Clarified Coached Communicated Coordinated Demystified Developed Enabled Encouraged Evaluated Explained Facilitated Guided Informed Instructed Persuaded Set Goals Stimulated Studied Taught Trained QUANTITATIVE Administered Allocated Analyzed Appraised Audited Balanced Budgeted Calculated Computed Developed Forecasted Managed Marketed Maximized Minimized Planned Projected Researched CREATIVE Acted Composed Conceived Conceptualized Created Customized Designed Developed Directed Established Fashioned Founded Illustrated Initiated Instituted Integrated Introduced Invented Originated Performed Planned Published Redesigned Revised Revitalized Shaped Visualized HELPING Assessed Assisted Clarified Coached Counseled Demonstrated Diagnosed Educated\\ Enhanced Expedited Facilitated Familiarized Guided Motivated Participated Proposed Provided Referred Rehabilitated Represented Served Supported ORGANIZATIONAL\\ Approved Accelerated Added Arranged Broadened Cataloged Centralized Changed Classified Collected Compiled Completed Controlled Defined Dispatched Executed Expanded Gained Gathered Generated Implemented Inspected Launched Monitored Operated Organized Prepared Processed Purchased Recorded Reduced Reinforced Retrieved Screened Selected Simplified Sold Specified Steered Structured Systematized Tabulated Unified Updated Utilized Validated Verified\"\npatterns = \"Administer Advise Analyzes Approve Arranges Assesses Assigns Assists Attends Audits Authorizes Collaborate Collects1 Communicate Compile Conduct Confers Confirms Consolidates Consult Coordinates1 Counsel Create Delegate Deliver Designs Develop Direct Disseminates Distribute Documents1 Draft Edit Educate Establish Estimate Evaluate Examines Facilitates Formulate Gather Guide Implement Inform Initiates Integrates Interact Interpret Investigates Issue Maintains Manage Modifies Monitors Motivate Negotiate Obtain Order Organizes Oversees1 Participate Plan Prepares Present Processes1 Produces Provide Recommends Reconcile Records Recruit Research Responds Review Scans Schedules Searches Selects Serves Solicit Solve Submit Supervise Supply Test Train Translate Verifies\"\n#p2 = \" \".join([i for i in final_words_list2])+patterns+p\nwith open(\"/kaggle/input/jobsextractor/working_words.txt\", 'r') as f:\n        patterns = f.read()\nwith open(\"/kaggle/input/jobsextractor/lda_words.txt\", 'r') as f1:\n        patterns2= f1.read()\n\nwith open(\"/kaggle/input/jobsextractor/j2.txt\", 'r') as f2:\n        file = f2.read()\n\npattern4 = set(file.split(\"\\n\"))\n\ndata = [\n              (\"Name of the Posts: Programmer \", {\"entities\": [(19, 29, \"JOB\")]}),\n              (\"Requirement for analyst part time in google\", {\"entities\": [(16, 23, \"JOB\")]}),\n              (\"Job posting for a writter\", {\"entities\": [(18, 25, \"JOB\")]}),\n              (\"vacancy for a manager in tata industries\", {\"entities\": [(14,21, \"JOB\")]}),\n              (\"posting for a intern in IIT bhu\", {\"entities\": [(14,20, \"JOB\")]}),\n              (\"vacancy for a research intern\", {\"entities\": [(14,22, \"JOB\")]}),\n              (\"required a technician for chemistry lab\", {\"entities\": [(11,21, \"JOB\")]}),\n              (\"temprary requirement for research fellow urgently\", {\"entities\": [(34,40, \"JOB\")]}),\n              (\"position for senior journslist in ABP News\", {\"entities\": [(20,30, \"JOB\")]}),\n              (\"employment for a engineer needed urgently\", {\"entities\": [(17,25, \"JOB\")]}),\n              (\"medical traineer at aiims delhi part time reqiured\", {\"entities\": [(8,16, \"JOB\")]}),\n              (\"post for a screwdriver endevour is empty from our neighbour\", {\"entities\": [(11,22, \"JOB\")]}),\n              (\"posting for a computer engineer job in microsoft\", {\"entities\": [(23,31, \"JOB\")]}),\n              (\"profession required is a manager in JSW\", {\"entities\": [(25,32, \"JOB\")]}),\n              (\"opening for a web developer in india\", {\"entities\": [(18,27, \"JOB\")]})\n              ]\n## p6 = working_2_10_dec, p5 = working_10_dec\np6 = [\"agile\", \"deadline-oriented\", \"multitask\", \"pressure\",\"multitasking\", \"enthusiastic\", \"high energy\", \"committed\", \"proactive\", \"pressure\", \"independently\", \"entrepreneurial\", \"independent\", \"resourceful\"]\np51 = \"warehouse support Accounting Human resources Warehouse Sales Manager Data entry Administrative Retail Executive assistant Project manager Medical Assistant Marketing Accountant Cashier Registered nurse Business analyst Office IT Warehouse worker Office manager Finance Mechanical engineer Construction Entry level Clerical Controller Engineer Manufacturing Accounts payable Paralegal Forklift operator Customer service representative LPN Call center Graphic designer Information technology Office assistant Maintenance Full time Customer services representative Driver Operations manager Data analyst Part-time Nurse Security Healthcare Bookkeeper Remote Analyst Pharmacist RN Sales representative Management Welder Payroll Office clerk Supervisor Nurse practitioner Attorney Purchasing Recruiter Financial analyst Software engineer Director Logistics Sales manager Electrician Server Banking Delivery driver Medical office receptionist Truck driver Assistant Legal Warehouse manager Insurance Teacher Education Real estate Secretary Engineering Account manager Medical Production supervisor Bartender CnA Buyer Maintenance technician Graphic design Automotive Accounts receivable Security officer Restaurant\"\np5 = \"essential salary necessary desirable applicant strong background qualification overtime experience worked knowlegde interview applicants immediate opening required flexible worked working skill skills role roles key full-time part-time well-paid badly paid high-powered stressful challenging rewarding repetitive glamorous plan years experience worked willing knowledge interview applicants interview immediate interested opening responsiblity resposiblities Administrative assistant Customer service Receptionist Part time UPS package handler part time entry level\"","metadata":{"execution":{"iopub.status.busy":"2022-03-08T07:19:13.123753Z","iopub.execute_input":"2022-03-08T07:19:13.124013Z","iopub.status.idle":"2022-03-08T07:19:13.155564Z","shell.execute_reply.started":"2022-03-08T07:19:13.123983Z","shell.execute_reply":"2022-03-08T07:19:13.154670Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#p51_ = []\n#for i in c.split(\" \"):\n#    p51_.append(r'\\b(?i)'+str(i)+r'\\b')","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:03:24.886384Z","iopub.execute_input":"2022-03-08T04:03:24.886865Z","iopub.status.idle":"2022-03-08T04:03:24.891543Z","shell.execute_reply.started":"2022-03-08T04:03:24.886831Z","shell.execute_reply":"2022-03-08T04:03:24.890356Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#inp = data_science_dataset['job_description'][5]\npath  =\"/kaggle/input/jobsextractor/Careers-Sample-Job-Ad.pdf\"\nspacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\ntext_from_pdf = spacy_.pdf_to_text()\nclean_text = spacy_.cleaning_texts(text_from_pdf)\npattern = p5+ \"\".join([i for i in p6]) #\"\".join([i for i in common_words2]) \nnlp, ruler = spacy_.nlp_model_initalization()\npat = spacy_.get_create_patterns(pattern)\ndes , list_des = spacy_.get_job_resume_discription(text_from_pdf, pat)\nsal = spacy_.get_salary(text_from_pdf)\nskills_required = spacy_.get_description_skill(nlp, text_from_pdf)\nnumber_of_post = spacy_.get_number_of_post(text_from_pdf)\njobs = trained_spacy_model_jobs(nlp, data)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T07:19:29.004544Z","iopub.execute_input":"2022-03-08T07:19:29.004936Z","iopub.status.idle":"2022-03-08T07:19:42.481865Z","shell.execute_reply.started":"2022-03-08T07:19:29.004895Z","shell.execute_reply":"2022-03-08T07:19:42.480772Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Losses {'ner': 2.0750840734884264}\nLosses {'ner': 5.48075427519646}\nLosses {'ner': 9.471210713690848}\nLosses {'ner': 11.733159636746567}\nLosses {'ner': 13.73074648893471}\nLosses {'ner': 15.407379197817686}\nLosses {'ner': 18.231001924587616}\nLosses {'ner': 20.256475861930284}\nLosses {'ner': 21.611358124309874}\nLosses {'ner': 23.49360814096329}\nLosses {'ner': 26.517193463214454}\nLosses {'ner': 28.257835607456798}\nLosses {'ner': 32.166439131994}\nLosses {'ner': 35.43414988309118}\nLosses {'ner': 37.79572033663183}\nLosses {'ner': 1.8232230605435689}\nLosses {'ner': 3.217793420212729}\nLosses {'ner': 4.941301140291671}\nLosses {'ner': 6.664891529894135}\nLosses {'ner': 8.90102426710689}\nLosses {'ner': 10.531439870930967}\nLosses {'ner': 11.951671510134815}\nLosses {'ner': 13.537469226638109}\nLosses {'ner': 14.654934257811306}\nLosses {'ner': 16.719560959089335}\nLosses {'ner': 19.034451912452255}\nLosses {'ner': 21.00115145696725}\nLosses {'ner': 22.551774673895466}\nLosses {'ner': 23.97766435104529}\nLosses {'ner': 25.294023462735254}\nLosses {'ner': 1.7125129316409584}\nLosses {'ner': 3.548628754069796}\nLosses {'ner': 5.163693323818734}\nLosses {'ner': 7.212874870368978}\nLosses {'ner': 8.598907372070244}\nLosses {'ner': 9.838394060818246}\nLosses {'ner': 11.735921541781863}\nLosses {'ner': 13.04435646123602}\nLosses {'ner': 14.059081372892251}\nLosses {'ner': 15.9925885181583}\nLosses {'ner': 16.771148899075342}\nLosses {'ner': 18.964810862671584}\nLosses {'ner': 20.963400974404067}\nLosses {'ner': 22.163151734042913}\nLosses {'ner': 23.305230871308595}\nLosses {'ner': 2.039921957766637}\nLosses {'ner': 3.1931490791612305}\nLosses {'ner': 3.8999734514509328}\nLosses {'ner': 5.035177538171411}\nLosses {'ner': 5.828961918654386}\nLosses {'ner': 7.660657479416841}\nLosses {'ner': 8.567659399461263}\nLosses {'ner': 9.003482689924567}\nLosses {'ner': 9.643047959598334}\nLosses {'ner': 10.941255072943022}\nLosses {'ner': 11.465594967561628}\nLosses {'ner': 11.889579485552531}\nLosses {'ner': 12.302768937461224}\nLosses {'ner': 13.562076052305201}\nLosses {'ner': 14.262019085112115}\nLosses {'ner': 0.41195759713446023}\nLosses {'ner': 1.2702346210891164}\nLosses {'ner': 1.7418975552747042}\nLosses {'ner': 2.398392236357836}\nLosses {'ner': 2.8107453004005265}\nLosses {'ner': 2.935759505730971}\nLosses {'ner': 4.45935355452059}\nLosses {'ner': 5.516581114817768}\nLosses {'ner': 6.826991970450422}\nLosses {'ner': 6.981527066816511}\nLosses {'ner': 7.335290966704292}\nLosses {'ner': 7.570624987961878}\nLosses {'ner': 7.803072228912924}\nLosses {'ner': 9.79464779004229}\nLosses {'ner': 10.059616344276264}\nLosses {'ner': 0.08327779920000467}\nLosses {'ner': 0.2591428494077881}\nLosses {'ner': 1.2129390438066074}\nLosses {'ner': 2.2123632781329343}\nLosses {'ner': 2.430156178195184}\nLosses {'ner': 3.60432871960601}\nLosses {'ner': 5.327199580842528}\nLosses {'ner': 5.439128235443044}\nLosses {'ner': 7.899948094217032}\nLosses {'ner': 8.113625015229088}\nLosses {'ner': 8.839297648585278}\nLosses {'ner': 9.320868511288703}\nLosses {'ner': 9.326566238799282}\nLosses {'ner': 9.803541388620062}\nLosses {'ner': 10.380134033085481}\nLosses {'ner': 1.1400434986345696}\nLosses {'ner': 1.1612637492281237}\nLosses {'ner': 1.161471192497905}\nLosses {'ner': 1.1624801608339144}\nLosses {'ner': 1.1826235299760348}\nLosses {'ner': 1.3409120821020433}\nLosses {'ner': 1.3432574742252688}\nLosses {'ner': 1.5035067303359404}\nLosses {'ner': 1.5052936591853965}\nLosses {'ner': 3.058161662147102}\nLosses {'ner': 3.0673430581023213}\nLosses {'ner': 3.124491143469464}\nLosses {'ner': 3.180806460390504}\nLosses {'ner': 3.181033678788912}\nLosses {'ner': 3.832585818287531}\nLosses {'ner': 0.09778057781165911}\nLosses {'ner': 0.10793509231658292}\nLosses {'ner': 0.10832341820045249}\nLosses {'ner': 2.0979317448112753}\nLosses {'ner': 2.1065369453260363}\nLosses {'ner': 2.115861020954339}\nLosses {'ner': 2.115910133733587}\nLosses {'ner': 2.1159230448089756}\nLosses {'ner': 2.1173796804377663}\nLosses {'ner': 2.124677933903188}\nLosses {'ner': 2.14343493110803}\nLosses {'ner': 2.143535644920067}\nLosses {'ner': 3.1583408510482043}\nLosses {'ner': 3.1673846225933944}\nLosses {'ner': 3.6964582393019185}\nLosses {'ner': 2.470158434895265e-05}\nLosses {'ner': 2.7769293546993026e-05}\nLosses {'ner': 0.17545997888321754}\nLosses {'ner': 0.1805706722682558}\nLosses {'ner': 0.18112996057292688}\nLosses {'ner': 2.1336992297991966}\nLosses {'ner': 2.2044639950231852}\nLosses {'ner': 2.204464021361584}\nLosses {'ner': 3.879108072892086}\nLosses {'ner': 3.9385337613946834}\nLosses {'ner': 3.938533768427025}\nLosses {'ner': 6.232961103689327}\nLosses {'ner': 6.233318387940024}\nLosses {'ner': 6.233326383833388}\nLosses {'ner': 6.299372067120481}\nLosses {'ner': 0.0002746344665175767}\nLosses {'ner': 0.00027464998321367185}\nLosses {'ner': 0.000274730886075682}\nLosses {'ner': 0.0003134587664068178}\nLosses {'ner': 0.007071078432838849}\nLosses {'ner': 0.10544483215948758}\nLosses {'ner': 0.11032699364521696}\nLosses {'ner': 0.11032703728949868}\nLosses {'ner': 0.11034030005550256}\nLosses {'ner': 0.16362144712068494}\nLosses {'ner': 0.16363961089331336}\nLosses {'ner': 0.16363977194411145}\nLosses {'ner': 0.2042906721662329}\nLosses {'ner': 0.2059209810694182}\nLosses {'ner': 2.065618872783996}\nLosses {'ner': 4.830253871648703e-05}\nLosses {'ner': 1.6797258478608919}\nLosses {'ner': 1.6797626667378458}\nLosses {'ner': 1.6797630413456446}\nLosses {'ner': 3.1376941939687577}\nLosses {'ner': 3.137694775425804}\nLosses {'ner': 3.1394260874758593}\nLosses {'ner': 3.152148725371844}\nLosses {'ner': 3.1521488053280335}\nLosses {'ner': 3.152183259008009}\nLosses {'ner': 5.137956609279753}\nLosses {'ner': 5.137956613885692}\nLosses {'ner': 5.17358702751872}\nLosses {'ner': 5.173587067615019}\nLosses {'ner': 5.173615266860355}\nLosses {'ner': 6.869547162926972e-10}\nLosses {'ner': 0.0021540200622027906}\nLosses {'ner': 0.07865583949557661}\nLosses {'ner': 0.07865584228561086}\nLosses {'ner': 0.07868783519853358}\nLosses {'ner': 0.08049733943092126}\nLosses {'ner': 0.08810286141934082}\nLosses {'ner': 0.08810286235150969}\nLosses {'ner': 0.09011519214559231}\nLosses {'ner': 0.09452231174950387}\nLosses {'ner': 0.09452374165591713}\nLosses {'ner': 1.0812862653078994}\nLosses {'ner': 1.0812877172538549}\nLosses {'ner': 1.0812878480724089}\nLosses {'ner': 1.0812882687662129}\nLosses {'ner': 0.2545924618744651}\nLosses {'ner': 0.29298815886304885}\nLosses {'ner': 0.2929892366092156}\nLosses {'ner': 0.29299161680277125}\nLosses {'ner': 0.29299161718649813}\nLosses {'ner': 0.2932031953548111}\nLosses {'ner': 0.2947214449078973}\nLosses {'ner': 0.2947257249639373}\nLosses {'ner': 0.2947257249930585}\nLosses {'ner': 0.29516680821240054}\nLosses {'ner': 0.2951841300901605}\nLosses {'ner': 0.29518450099890964}\nLosses {'ner': 0.2963351154291693}\nLosses {'ner': 0.29636000054487077}\nLosses {'ner': 0.2963600006549109}\nLosses {'ner': 6.41267541275185e-08}\nLosses {'ner': 0.0001675279963455225}\nLosses {'ner': 0.00037015948973831273}\nLosses {'ner': 0.0003701600322125835}\nLosses {'ner': 1.303633347496872}\nLosses {'ner': 1.3036333475163864}\nLosses {'ner': 1.3036392121348195}\nLosses {'ner': 1.3036395955500262}\nLosses {'ner': 1.3037148147124278}\nLosses {'ner': 1.3037148924418798}\nLosses {'ner': 1.303714893970067}\nLosses {'ner': 1.3037168495248384}\nLosses {'ner': 1.303716850014589}\nLosses {'ner': 1.3037203560407977}\nLosses {'ner': 1.303720359950271}\nLosses {'ner': 0.00044055927359824804}\nLosses {'ner': 0.00044064319722026286}\nLosses {'ner': 0.00044908320834894045}\nLosses {'ner': 0.000510735834461324}\nLosses {'ner': 0.0005107358429535148}\nLosses {'ner': 0.0005107403207906312}\nLosses {'ner': 0.0005107404002606871}\nLosses {'ner': 0.0005107404305523439}\nLosses {'ner': 0.0005112422904000088}\nLosses {'ner': 0.0005112425990592205}\nLosses {'ner': 0.0005137512470999344}\nLosses {'ner': 0.0005137540036488935}\nLosses {'ner': 0.0005137543141027059}\nLosses {'ner': 0.5816117497261895}\nLosses {'ner': 0.5816117626458632}\nLosses {'ner': 3.1828906463742674e-09}\nLosses {'ner': 1.9059727223770253}\nLosses {'ner': 1.9415510030656153}\nLosses {'ner': 1.9415510039647272}\nLosses {'ner': 1.9415510039656545}\nLosses {'ner': 1.9415510039677122}\nLosses {'ner': 1.9415867961252045}\nLosses {'ner': 1.9415867961252091}\nLosses {'ner': 1.9415906563418093}\nLosses {'ner': 1.9415910631425177}\nLosses {'ner': 1.9415910763450808}\nLosses {'ner': 3.379069982832149}\nLosses {'ner': 3.379069983146208}\nLosses {'ner': 3.379069986385157}\nLosses {'ner': 3.3790699951776486}\nLosses {'ner': 7.383209742634341e-08}\nLosses {'ner': 1.428095159830711}\nLosses {'ner': 1.4281097316438651}\nLosses {'ner': 1.4281097317427216}\nLosses {'ner': 1.4281106103317662}\nLosses {'ner': 1.4283624860397035}\nLosses {'ner': 1.4283625320602011}\nLosses {'ner': 1.428362532137734}\nLosses {'ner': 1.4283625321377553}\nLosses {'ner': 2.117387338203969}\nLosses {'ner': 2.1173873382396735}\nLosses {'ner': 2.1173877172125803}\nLosses {'ner': 2.1173877174230062}\nLosses {'ner': 2.1173888782924135}\nLosses {'ner': 2.117388878293023}\nLosses {'ner': 3.4604270553012414e-09}\nLosses {'ner': 1.621696775033162e-05}\nLosses {'ner': 1.622085639393196e-05}\nLosses {'ner': 1.753489080318417e-05}\nLosses {'ner': 1.7534893246236555e-05}\nLosses {'ner': 1.754018767420229e-05}\nLosses {'ner': 0.00645263824166762}\nLosses {'ner': 0.006452638241667996}\nLosses {'ner': 0.006452642797228544}\nLosses {'ner': 0.006452654728957205}\nLosses {'ner': 0.006455199397821289}\nLosses {'ner': 0.006455199446323602}\nLosses {'ner': 0.006455200178530319}\nLosses {'ner': 0.0064568329098143125}\nLosses {'ner': 0.046744610691301924}\nLosses {'ner': 5.888022989626681e-05}\nLosses {'ner': 0.0037209970113899304}\nLosses {'ner': 0.003721009001672172}\nLosses {'ner': 0.00987877811016718}\nLosses {'ner': 0.009878778694819403}\nLosses {'ner': 0.009878778719369891}\nLosses {'ner': 0.009878979358400989}\nLosses {'ner': 0.009878979360735379}\nLosses {'ner': 0.009901313490959034}\nLosses {'ner': 0.009901313644585618}\nLosses {'ner': 0.009901313724758009}\nLosses {'ner': 0.009901313724772341}\nLosses {'ner': 2.0048607874367264}\nLosses {'ner': 2.0048607874414484}\nLosses {'ner': 2.022942240148807}\nLosses {'ner': 0.30293661355979257}\nLosses {'ner': 0.30293661355981766}\nLosses {'ner': 0.30293661671866434}\nLosses {'ner': 0.30293661674613326}\nLosses {'ner': 0.3029366170129772}\nLosses {'ner': 0.3029366170129828}\nLosses {'ner': 0.3105500817554367}\nLosses {'ner': 0.3105500818217501}\nLosses {'ner': 0.3105500820526094}\nLosses {'ner': 0.3105501407468818}\nLosses {'ner': 2.143932518158296}\nLosses {'ner': 2.1439325181583566}\nLosses {'ner': 2.1439325201817128}\nLosses {'ner': 2.1439325201820663}\nLosses {'ner': 4.035569011004025}\nLosses {'ner': 0.0004900696222164346}\nLosses {'ner': 0.0004900696310994354}\nLosses {'ner': 0.003153079931363831}\nLosses {'ner': 0.00315308006624638}\nLosses {'ner': 0.0031530915432745406}\nLosses {'ner': 0.0031532253581271095}\nLosses {'ner': 0.0031532262885411045}\nLosses {'ner': 0.003153227249488558}\nLosses {'ner': 0.003175252579303494}\nLosses {'ner': 0.003175252590022648}\nLosses {'ner': 0.0031826759149104207}\nLosses {'ner': 0.0032899413667925002}\nLosses {'ner': 0.0032901888639468753}\nLosses {'ner': 0.27172068958092477}\nLosses {'ner': 0.2717206895809272}\nLosses {'ner': 1.933527198094868e-12}\nLosses {'ner': 2.1157620840539103e-07}\nLosses {'ner': 2.1157658251492447e-07}\nLosses {'ner': 2.1157762396577609e-07}\nLosses {'ner': 0.0009716267871433968}\nLosses {'ner': 0.0009716268178237827}\nLosses {'ner': 0.0009716268188272833}\nLosses {'ner': 0.0009716268188710992}\nLosses {'ner': 0.001048213751767392}\nLosses {'ner': 0.001048226673729374}\nLosses {'ner': 0.0010482449195260133}\nLosses {'ner': 0.0016536166255776362}\nLosses {'ner': 0.001653633660638009}\nLosses {'ner': 0.28516118938382856}\nLosses {'ner': 0.28516126155640825}\nLosses {'ner': 0.036867984080096755}\nLosses {'ner': 0.03686798408588429}\nLosses {'ner': 0.036867984572302726}\nLosses {'ner': 0.03686839691979105}\nLosses {'ner': 0.03686839692075453}\nLosses {'ner': 0.036868399579301256}\nLosses {'ner': 0.0368683995793424}\nLosses {'ner': 1.7146982807012514}\nLosses {'ner': 1.7146983912613387}\nLosses {'ner': 1.714698391280742}\nLosses {'ner': 1.7146983918895937}\nLosses {'ner': 1.7146997188434736}\nLosses {'ner': 1.7146999422493363}\nLosses {'ner': 1.7146999423787794}\nLosses {'ner': 1.7146999423949663}\nLosses {'ner': 1.639716770155895e-10}\nLosses {'ner': 3.0833405393023967e-06}\nLosses {'ner': 3.084142339814712e-06}\nLosses {'ner': 4.141081679756204e-06}\nLosses {'ner': 4.141589405927245e-06}\nLosses {'ner': 4.141594633710471e-06}\nLosses {'ner': 4.141634067759605e-06}\nLosses {'ner': 4.141634110154795e-06}\nLosses {'ner': 4.141634178360507e-06}\nLosses {'ner': 4.14165618145951e-06}\nLosses {'ner': 1.5490146100411313}\nLosses {'ner': 1.5490161483624445}\nLosses {'ner': 1.5490161483722975}\nLosses {'ner': 3.5531089471458355}\nLosses {'ner': 3.5543345233143286}\nLosses {'ner': 1.713570153748234e-11}\nLosses {'ner': 1.7280334800747296e-11}\nLosses {'ner': 1.7280722127556098e-11}\nLosses {'ner': 6.053262815854798e-10}\nLosses {'ner': 6.801322105066081e-09}\nLosses {'ner': 2.8112400932518445e-05}\nLosses {'ner': 2.8583146391050236e-05}\nLosses {'ner': 2.858316838291056e-05}\nLosses {'ner': 2.8588552672685013e-05}\nLosses {'ner': 2.8588553360971494e-05}\nLosses {'ner': 2.873147735158129e-05}\nLosses {'ner': 2.873147735894702e-05}\nLosses {'ner': 2.873199500672187e-05}\nLosses {'ner': 2.941913035437651e-05}\nLosses {'ner': 3.59954276689719e-05}\nLosses {'ner': 3.250036019854379e-08}\nLosses {'ner': 0.002343970726485614}\nLosses {'ner': 0.0023439707266391294}\nLosses {'ner': 0.0023439707266644026}\nLosses {'ner': 0.0023748817326295366}\nLosses {'ner': 0.00237488174344317}\nLosses {'ner': 0.002374902720336874}\nLosses {'ner': 0.0025334796835992063}\nLosses {'ner': 0.0025335094860139016}\nLosses {'ner': 0.002533509488046994}\nLosses {'ner': 0.0025335096966982345}\nLosses {'ner': 0.002533509696699851}\nLosses {'ner': 0.0025335108091734757}\nLosses {'ner': 0.0025335886882231515}\nLosses {'ner': 0.0025335888182308986}\nLosses {'ner': 0.03242883459123616}\nLosses {'ner': 0.03242883460123272}\nLosses {'ner': 0.0324288346026009}\nLosses {'ner': 0.03242883460261468}\nLosses {'ner': 0.03242883460266075}\nLosses {'ner': 0.03242883460267505}\nLosses {'ner': 0.03242883460267958}\nLosses {'ner': 0.032428861145369}\nLosses {'ner': 0.03242886367505345}\nLosses {'ner': 0.03242886369293245}\nLosses {'ner': 0.03242886759997053}\nLosses {'ner': 0.0324296486674762}\nLosses {'ner': 0.03242965187358813}\nLosses {'ner': 0.032429651911169886}\nLosses {'ner': 0.03242965192863618}\nLosses {'ner': 1.1761497389578376e-07}\nLosses {'ner': 0.006798967997490163}\nLosses {'ner': 0.03849168364354955}\nLosses {'ner': 0.03849168454865807}\nLosses {'ner': 0.038494243440665696}\nLosses {'ner': 0.038494243440669464}\nLosses {'ner': 0.038494245379029575}\nLosses {'ner': 0.03849424537972339}\nLosses {'ner': 0.038494245379799306}\nLosses {'ner': 0.03849424541039311}\nLosses {'ner': 0.038494270137252824}\nLosses {'ner': 0.0384942713601786}\nLosses {'ner': 0.03849427136017937}\nLosses {'ner': 1.3368996802194324}\nLosses {'ner': 1.3368996822750483}\nLosses {'ner': 5.18661377352199e-12}\nLosses {'ner': 0.0034782174019062397}\nLosses {'ner': 0.0034782223045424464}\nLosses {'ner': 0.003478222413046718}\nLosses {'ner': 0.003478222413046718}\nLosses {'ner': 0.6354865365457373}\nLosses {'ner': 0.6354866271728002}\nLosses {'ner': 0.635486627233421}\nLosses {'ner': 0.635486627233421}\nLosses {'ner': 0.6354866285963555}\nLosses {'ner': 0.6354866323104028}\nLosses {'ner': 0.635486632310865}\nLosses {'ner': 0.6354866323108653}\nLosses {'ner': 0.6354866323246778}\nLosses {'ner': 0.6354984565306914}\nLosses {'ner': 6.344973384706075e-05}\nLosses {'ner': 6.37329700371609e-05}\nLosses {'ner': 6.383029138560356e-05}\nLosses {'ner': 6.383030373054468e-05}\nLosses {'ner': 6.38303037449801e-05}\nLosses {'ner': 0.00015337469652411574}\nLosses {'ner': 0.0001533746981688327}\nLosses {'ner': 0.00015337647393078423}\nLosses {'ner': 0.00015337651112458714}\nLosses {'ner': 0.00015337656110933534}\nLosses {'ner': 0.00015337656110994577}\nLosses {'ner': 0.00015337656115252846}\nLosses {'ner': 0.00015337656201919902}\nLosses {'ner': 0.0001533765911327077}\nLosses {'ner': 0.00015338930675112063}\n","output_type":"stream"}]},{"cell_type":"code","source":"#cols = ['job', 'skills_required', 'salary', 'job_description', 'salary']\n\n@app.route('/')\ndef home():\n    return render_template(\"home.html\")\n\n@app.route('/predict',methods=['POST'])\ndef predict():\n    path  =\"/kaggle/input/jobsextractor/Careers-Sample-Job-Ad.pdf\"\n    spacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\n    text_from_pdf = spacy_.pdf_to_text()\n    clean_text = spacy_.cleaning_texts(text_from_pdf)\n    pattern = p5+ \"\".join([i for i in p6]) #\"\".join([i for i in common_words2]) \n    nlp, ruler = spacy_.nlp_model_initalization()\n    pat = spacy_.get_create_patterns(pattern)\n    des , list_des = spacy_.get_job_resume_discription(text_from_pdf, pat)\n    sal = spacy_.get_salary(text_from_pdf)\n    skills_required = spacy_.get_description_skill(nlp, text_from_pdf)\n    number_of_post = spacy_.get_number_of_post(text_from_pdf)\n    jobs = trained_spacy_model_jobs(nlp, data)\n    predicon = [des, skiils_required , number_of_posts, jobs, sal]\n    res = render_template('home.html',pred='Expected Bill will be {}'.format(prediction))\n\n\n\nif __name__ == '__main__':\n    app.run(debug=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T07:32:17.992381Z","iopub.execute_input":"2022-03-08T07:32:17.992619Z","iopub.status.idle":"2022-03-08T07:32:20.065175Z","shell.execute_reply.started":"2022-03-08T07:32:17.992595Z","shell.execute_reply":"2022-03-08T07:32:20.064055Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":" * Serving Flask app '__main__' (lazy loading)\n * Environment: production\n\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n\u001b[2m   Use a production WSGI server instead.\u001b[0m\n * Debug mode: on\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 845, in launch_instance\n    app.initialize(argv)\n  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 88, in inner\n    return method(app, *args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 632, in initialize\n    self.init_sockets()\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 282, in init_sockets\n    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 229, in _bind_socket\n    return self._try_bind_socket(s, port)\n  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 205, in _try_bind_socket\n    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n  File \"/opt/conda/lib/python3.7/site-packages/zmq/sugar/socket.py\", line 214, in bind\n    super().bind(addr)\n  File \"zmq/backend/cython/socket.pyx\", line 540, in zmq.backend.cython.socket.Socket.bind\n  File \"zmq/backend/cython/checkrc.pxd\", line 28, in zmq.backend.cython.checkrc._check_rc\nzmq.error.ZMQError: Address already in use\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"],"ename":"SystemExit","evalue":"1","output_type":"error"}]},{"cell_type":"code","source":"#job = set(spacy_.get_job_from_training_spacy_model(data, nlp, text_from_pdf))","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:58:32.758614Z","iopub.execute_input":"2022-03-08T04:58:32.758955Z","iopub.status.idle":"2022-03-08T04:58:32.764422Z","shell.execute_reply.started":"2022-03-08T04:58:32.758919Z","shell.execute_reply":"2022-03-08T04:58:32.762852Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"doc = nlp(text_from_pdf)\nprint([(ent.text, ent.label_) for ent in doc.ents])","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:58:57.022282Z","iopub.execute_input":"2022-03-08T04:58:57.022640Z","iopub.status.idle":"2022-03-08T04:58:57.162219Z","shell.execute_reply.started":"2022-03-08T04:58:57.022608Z","shell.execute_reply":"2022-03-08T04:58:57.160939Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"[('Consultant', 'JOB'), ('Consultant', 'JOB'), ('research', 'JOB'), ('documentation', 'SKILL|documentation'), ('research', 'JOB'), ('documentation', 'SKILL|documentation'), ('Support', 'SKILL|support'), ('project management', 'SKILL|project-management'), ('monitoring', 'SKILL|monitoring'), ('marketing', 'SKILL|marketing'), ('support', 'SKILL|support'), ('project management', 'SKILL|project-management')]\n","output_type":"stream"}]},{"cell_type":"code","source":"#print(text_from_pdf)\nfor i in doc.ents:\n    print(i.label_, i.text)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T04:58:59.852882Z","iopub.execute_input":"2022-03-08T04:58:59.853206Z","iopub.status.idle":"2022-03-08T04:58:59.864051Z","shell.execute_reply.started":"2022-03-08T04:58:59.853173Z","shell.execute_reply":"2022-03-08T04:58:59.862924Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"JOB Consultant\nJOB Consultant\nJOB research\nSKILL|documentation documentation\nJOB research\nSKILL|documentation documentation\nSKILL|support Support\nSKILL|project-management project management\nSKILL|monitoring monitoring\nSKILL|marketing marketing\nSKILL|support support\nSKILL|project-management project management\n","output_type":"stream"}]},{"cell_type":"code","source":"output_dir = Path('/kaggle/working/nlp-job')\nnlp.to_disk(output_dir)\n\nnlp_updated = spacy.load(output_dir)\ndoc1 = nlp_updated(text_from_pdf)\n\nfor i in doc1.ents:\n    print(i.label_, i.text)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T05:04:25.422476Z","iopub.execute_input":"2022-03-08T05:04:25.422858Z","iopub.status.idle":"2022-03-08T05:04:30.336841Z","shell.execute_reply.started":"2022-03-08T05:04:25.422817Z","shell.execute_reply":"2022-03-08T05:04:30.336043Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"JOB Consultant\nJOB Consultant\nJOB research\nSKILL|documentation documentation\nJOB research\nSKILL|documentation documentation\nSKILL|support Support\nSKILL|project-management project management\nSKILL|monitoring monitoring\nSKILL|marketing marketing\nSKILL|support support\nSKILL|project-management project management\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\ndef save_model_spacy(path, output_file_name):\n    output_dir = Path(path)\n    nlp.to_disk(output_dir)\n    shutil.make_archive(output_file_name, 'zip', output_dir)\n    \ndef load_updated_model(output_dir, text):\n    nlp_updated = spacy.load(output_dir)\n    doc1 = nlp_updated(text)\n    return doc1","metadata":{"execution":{"iopub.status.busy":"2022-03-08T05:15:50.572716Z","iopub.execute_input":"2022-03-08T05:15:50.573144Z","iopub.status.idle":"2022-03-08T05:16:45.406108Z","shell.execute_reply.started":"2022-03-08T05:15:50.573105Z","shell.execute_reply":"2022-03-08T05:16:45.405210Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/nlp_model.zip.zip'"},"metadata":{}}]},{"cell_type":"code","source":"import os \nos.chdir(r'/kaggle/working')\nfrom IPython.display import FileLinks \nFileLinks(r'nlp-job')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-08T05:09:54.606128Z","iopub.execute_input":"2022-03-08T05:09:54.606743Z","iopub.status.idle":"2022-03-08T05:09:54.617757Z","shell.execute_reply.started":"2022-03-08T05:09:54.606677Z","shell.execute_reply":"2022-03-08T05:09:54.616813Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"nlp-job/\n  meta.json\n  config.cfg\n  tokenizer\nnlp-job/attribute_ruler/\n  patterns\nnlp-job/entity_ruler/\n  patterns.jsonl\n  cfg\nnlp-job/lemmatizer/lookups/\n  lookups.bin\nnlp-job/ner/\n  model\n  moves\n  cfg\nnlp-job/parser/\n  model\n  moves\n  cfg\nnlp-job/senter/\n  model\n  cfg\nnlp-job/tagger/\n  model\n  cfg\nnlp-job/tok2vec/\n  model\n  cfg\nnlp-job/vocab/\n  vectors\n  lookups.bin\n  strings.json\n  key2row","text/html":"nlp-job/<br>\n&nbsp;&nbsp;<a href='nlp-job/meta.json' target='_blank'>meta.json</a><br>\n&nbsp;&nbsp;<a href='nlp-job/config.cfg' target='_blank'>config.cfg</a><br>\n&nbsp;&nbsp;<a href='nlp-job/tokenizer' target='_blank'>tokenizer</a><br>\nnlp-job/attribute_ruler/<br>\n&nbsp;&nbsp;<a href='nlp-job/attribute_ruler/patterns' target='_blank'>patterns</a><br>\nnlp-job/entity_ruler/<br>\n&nbsp;&nbsp;<a href='nlp-job/entity_ruler/patterns.jsonl' target='_blank'>patterns.jsonl</a><br>\n&nbsp;&nbsp;<a href='nlp-job/entity_ruler/cfg' target='_blank'>cfg</a><br>\nnlp-job/lemmatizer/lookups/<br>\n&nbsp;&nbsp;<a href='nlp-job/lemmatizer/lookups/lookups.bin' target='_blank'>lookups.bin</a><br>\nnlp-job/ner/<br>\n&nbsp;&nbsp;<a href='nlp-job/ner/model' target='_blank'>model</a><br>\n&nbsp;&nbsp;<a href='nlp-job/ner/moves' target='_blank'>moves</a><br>\n&nbsp;&nbsp;<a href='nlp-job/ner/cfg' target='_blank'>cfg</a><br>\nnlp-job/parser/<br>\n&nbsp;&nbsp;<a href='nlp-job/parser/model' target='_blank'>model</a><br>\n&nbsp;&nbsp;<a href='nlp-job/parser/moves' target='_blank'>moves</a><br>\n&nbsp;&nbsp;<a href='nlp-job/parser/cfg' target='_blank'>cfg</a><br>\nnlp-job/senter/<br>\n&nbsp;&nbsp;<a href='nlp-job/senter/model' target='_blank'>model</a><br>\n&nbsp;&nbsp;<a href='nlp-job/senter/cfg' target='_blank'>cfg</a><br>\nnlp-job/tagger/<br>\n&nbsp;&nbsp;<a href='nlp-job/tagger/model' target='_blank'>model</a><br>\n&nbsp;&nbsp;<a href='nlp-job/tagger/cfg' target='_blank'>cfg</a><br>\nnlp-job/tok2vec/<br>\n&nbsp;&nbsp;<a href='nlp-job/tok2vec/model' target='_blank'>model</a><br>\n&nbsp;&nbsp;<a href='nlp-job/tok2vec/cfg' target='_blank'>cfg</a><br>\nnlp-job/vocab/<br>\n&nbsp;&nbsp;<a href='nlp-job/vocab/vectors' target='_blank'>vectors</a><br>\n&nbsp;&nbsp;<a href='nlp-job/vocab/lookups.bin' target='_blank'>lookups.bin</a><br>\n&nbsp;&nbsp;<a href='nlp-job/vocab/strings.json' target='_blank'>strings.json</a><br>\n&nbsp;&nbsp;<a href='nlp-job/vocab/key2row' target='_blank'>key2row</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"number_of_post","metadata":{"execution":{"iopub.status.busy":"2021-12-14T04:11:26.336248Z","iopub.execute_input":"2021-12-14T04:11:26.336497Z","iopub.status.idle":"2021-12-14T04:11:26.343721Z","shell.execute_reply.started":"2021-12-14T04:11:26.336466Z","shell.execute_reply":"2021-12-14T04:11:26.342648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#c = \"position post place situation appointment posting work calling career trade capacity function occupation profession craft employment placement vocation pursuit activity billet field métier office opening station vacancy berth business connection job employ grip livelihood position employment engagement faculty field gig grind handicraft work nine-to-five racket spot line business earning \"\n#c.split(\" \")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T08:51:29.404355Z","iopub.execute_input":"2021-12-13T08:51:29.404629Z","iopub.status.idle":"2021-12-13T08:51:29.408221Z","shell.execute_reply.started":"2021-12-13T08:51:29.404603Z","shell.execute_reply":"2021-12-13T08:51:29.407716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(text_from_pdf)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T08:40:46.141597Z","iopub.execute_input":"2021-12-13T08:40:46.141849Z","iopub.status.idle":"2021-12-13T08:40:46.146545Z","shell.execute_reply.started":"2021-12-13T08:40:46.141814Z","shell.execute_reply":"2021-12-13T08:40:46.145337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith open(\"/kaggle/input/jobsextractor/skills_12-dec.txt\", 'r') as f:\n    skills_12_dec = f.read()\nskills_12_dec_1 = list(set(skills_12_dec.lower().split(\"\\n\")))\nskills_12_dec_2 = []\nfor i in skills_12_dec_1[1:]:\n        j = i.split(\" \")\n        if len(j) == 2:\n            skills_12_dec_2.append([{\"label\":\"SKILL|\"+str(i).replace(\" \",\"-\"),\"pattern\":[{\"LOWER\":str(j[0])},{\"LOWER\":str(j[1])}]}])\n        elif len(j) == 3:\n            skills_12_dec_2.append({\"label\":\"SKILL|\"+str(i).replace(\" \",\"-\"),\"pattern\":[{\"LOWER\":str(j[0])},{\"LOWER\":str(j[1])},{\"LOWER\":str(j[2])}]})\n        elif len(j) == 4:\n            skills_12_dec_2.append({\"label\":\"SKILL|\"+str(i).replace(\" \",\"-\"),\"pattern\":[{\"LOWER\":str(j[0])},{\"LOWER\":str(j[1])},{\"LOWER\":str(j[2])}, {\"LOWER\":str(j[3])}]})\n​\nfor i in skills_12_dec_2:\n    ruler.add_patterns([i])\n\n#skills_12_dec_2_ = \"/n\".join([str(i) for i in skills_12_dec_2])\nskills_12_dec_2_.replace(\"/n\", \" \")\n\nimport json\nwith open('/kaggle/working/skills_12_dec_3.jsonl', 'w') as f1:\n    json.dump(skills_12_dec_2, f1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T03:17:40.306345Z","iopub.execute_input":"2021-12-13T03:17:40.306662Z","iopub.status.idle":"2021-12-13T03:17:40.324643Z","shell.execute_reply.started":"2021-12-13T03:17:40.306622Z","shell.execute_reply":"2021-12-13T03:17:40.323895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Resume Extractor","metadata":{}},{"cell_type":"code","source":"path  =\"/kaggle/input/jobsextractor/sid.pdf\"\nspacy_ =  resume_spacy_pdf_clean_skills(path, \"specific_cleaning\" )\nnlp, ruler = spacy_.nlp_model_initalization()\ntext_from_pdf = spacy_.pdf_to_text()\nclean_text = spacy_.cleaning_texts(text_from_pdf)\nget_skills_from_resume, others= spacy_.get_skills(nlp,clean_text)\n#pat = spacy_.get_create_patterns(patterns)\n#description = spacy_.get_job_discription(text_from_pdf, pat)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:55:05.400373Z","iopub.execute_input":"2021-12-11T07:55:05.400693Z","iopub.status.idle":"2021-12-11T07:55:08.371958Z","shell.execute_reply.started":"2021-12-11T07:55:05.400657Z","shell.execute_reply":"2021-12-11T07:55:08.371097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"match = spacy_.get_matching_score(skills_required, get_skills_from_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T07:55:08.373844Z","iopub.execute_input":"2021-12-11T07:55:08.374145Z","iopub.status.idle":"2021-12-11T07:55:08.378474Z","shell.execute_reply.started":"2021-12-11T07:55:08.374102Z","shell.execute_reply":"2021-12-11T07:55:08.377809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_skills_from_text","metadata":{"execution":{"iopub.status.busy":"2021-12-10T08:46:27.079633Z","iopub.execute_input":"2021-12-10T08:46:27.080743Z","iopub.status.idle":"2021-12-10T08:46:27.087916Z","shell.execute_reply.started":"2021-12-10T08:46:27.08069Z","shell.execute_reply":"2021-12-10T08:46:27.08727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ruler = EntityRuler(nlp)\n#patterns = [{\"label\":\"ORG\",\"pattern\":\"Skills\"}]\n#ruler = nlp.add_pipe(\"entity_ruler\")\n#ruler.add_patterns(patterns)\n#ruler.patterns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spacy import displacy\n#sent = nlp(data[\"Resume_str\"].iloc[0])\n#displacy.render(sent, style=\"ent\", jupyter=True)\n\npatterns = data.Category.unique()\nfor a in patterns:\n    ruler.add_patterns([{\"label\": \"Job-Category\", \"pattern\": a}])\n    \noptions=[{\"ents\": \"Job-Category\", \"colors\": \"#ff3232\"},{\"ents\": \"SKILL\", \"colors\": \"#56c426\"}]\noptions=[{\"ents\": \"Job-Category\", \"colors\": \"#ff3232\"},{\"ents\": \"SKILL\", \"colors\": \"#56c426\"}]\ncolors = {\n    \"Job-Category\": \"linear-gradient(90deg, #aa9cfc, #fc9ce7)\",\n    \"SKILL\": \"linear-gradient(90deg, #9BE15D, #00E3AE)\",\n    \"ORG\": \"#ffd966\",\n    \"PERSON\": \"#e06666\",\n    \"GPE\": \"#9fc5e8\",\n    \"DATE\": \"#c27ba0\",\n    \"ORDINAL\": \"#674ea7\",\n    \"PRODUCT\": \"#f9cb9c\",\n}\noptions = {\n    \"ents\": [\n        \"Job-Category\",\n        \"SKILL\",\n        \"ORG\",\n        \"PERSON\",\n        \"GPE\",\n        \"DATE\",\n        \"ORDINAL\",\n        \"PRODUCT\",\n    ],\n    \"colors\": colors,\n}\n#sent = nlp(data[\"Resume_str\"].iloc[5])\n#displacy.render(sent, style=\"ent\", jupyter=True, options=options)\nsent2 = nlp(text_from_pdf)\ndisplacy.render(sent2, style=\"ent\", jupyter=True, options=options)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:38:55.085728Z","iopub.execute_input":"2021-12-09T06:38:55.086144Z","iopub.status.idle":"2021-12-09T06:38:55.487957Z","shell.execute_reply.started":"2021-12-09T06:38:55.086105Z","shell.execute_reply":"2021-12-09T06:38:55.486928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in sent2.sents:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T05:41:53.835308Z","iopub.execute_input":"2021-12-09T05:41:53.835652Z","iopub.status.idle":"2021-12-09T05:41:53.849546Z","shell.execute_reply.started":"2021-12-09T05:41:53.835619Z","shell.execute_reply":"2021-12-09T05:41:53.84862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in sent2.ents:\n    #print(i.label_)\n    if i.label_ == \"ORG\":\n        print(i.text)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T06:27:26.815328Z","iopub.execute_input":"2021-12-09T06:27:26.815789Z","iopub.status.idle":"2021-12-09T06:27:26.917352Z","shell.execute_reply.started":"2021-12-09T06:27:26.815642Z","shell.execute_reply":"2021-12-09T06:27:26.915848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning Text","metadata":{}},{"cell_type":"code","source":"import re\ndef clean_data(resumeText):\n    resumeText = re.sub('httpS+s*', ' ', resumeText)  # remove URLs\n    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc\n    resumeText = re.sub('#S+', '', resumeText)  # remove hashtags\n    #resumeText = re.sub('@S+', '  ', resumeText)  # remove mentions\n    resumeText = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[]^_`{|}~\"\"\"), ' ', resumeText)  # remove punctuations\n    resumeText = re.sub(r'[^x00-x7f]',r' ', resumeText) \n    #resumeText = re.sub('s+', ' ', resumeText)  # remove extra whitespace\n    return resumeText\n\n#data['cleaned'] = data['Resume_str'].apply(lambda x : clean_data(x))\n\n\ndef clean_t(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [lemmer.lemmatize(word) for word in tokens if word not in stopwords]\n    text_final  = \" \".join(i for i in text)\n    return text_final\n\n\n#data['cleaned2'] = data['cleaned'].apply(lambda x :  clean_t(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:36:38.853643Z","iopub.execute_input":"2021-12-10T03:36:38.853944Z","iopub.status.idle":"2021-12-10T03:36:38.863612Z","shell.execute_reply.started":"2021-12-10T03:36:38.853914Z","shell.execute_reply":"2021-12-10T03:36:38.86238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = []\nfor i in words:\n    w.append(clean_data(i))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:36:39.40888Z","iopub.execute_input":"2021-12-10T03:36:39.409206Z","iopub.status.idle":"2021-12-10T03:36:39.843329Z","shell.execute_reply.started":"2021-12-10T03:36:39.409176Z","shell.execute_reply":"2021-12-10T03:36:39.842336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w","metadata":{"execution":{"iopub.status.busy":"2021-12-10T03:36:39.965666Z","iopub.execute_input":"2021-12-10T03:36:39.966295Z","iopub.status.idle":"2021-12-10T03:36:39.991961Z","shell.execute_reply.started":"2021-12-10T03:36:39.966249Z","shell.execute_reply":"2021-12-10T03:36:39.991262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w = \" \".join([i for i in words])\nw2 = clean_data(w).split(\" \")\nw2","metadata":{"execution":{"iopub.status.busy":"2021-12-10T02:46:20.130076Z","iopub.execute_input":"2021-12-10T02:46:20.130368Z","iopub.status.idle":"2021-12-10T02:46:20.231125Z","shell.execute_reply.started":"2021-12-10T02:46:20.130334Z","shell.execute_reply":"2021-12-10T02:46:20.230213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning Dataset","metadata":{}},{"cell_type":"code","source":"\nimport re\nclean = []\nfor i in range(data.shape[0]):\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        data[\"Resume_str\"].iloc[i],\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)\n    clean.append(review)\ndata['cleaned-1'] = clean","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:32.788149Z","iopub.execute_input":"2021-12-09T03:43:32.788524Z","iopub.status.idle":"2021-12-09T03:43:48.779096Z","shell.execute_reply.started":"2021-12-09T03:43:32.78849Z","shell.execute_reply":"2021-12-09T03:43:48.777812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cleaning Selected file","metadata":{}},{"cell_type":"code","source":"def get_clean_file(text):\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        text,\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)\n    return review\n\nclean_text = get_clean_file(text_from_pdf)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:56:17.413685Z","iopub.execute_input":"2021-12-09T03:56:17.414018Z","iopub.status.idle":"2021-12-09T03:56:17.425249Z","shell.execute_reply.started":"2021-12-09T03:56:17.413984Z","shell.execute_reply":"2021-12-09T03:56:17.424478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_skills(text):\n    doc = nlp(text)\n    skills_ = []\n    for ent in doc.ents:\n        if \"SKILL\" in ent.label_ or ent.label_ == \"ORG\":\n            skills_.append(ent.text)\n    \n    return skills_\n\nget_skills_from_text = get_skills(clean_text)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:56:18.489838Z","iopub.execute_input":"2021-12-09T03:56:18.490643Z","iopub.status.idle":"2021-12-09T03:56:18.63054Z","shell.execute_reply.started":"2021-12-09T03:56:18.490586Z","shell.execute_reply":"2021-12-09T03:56:18.629575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_skills_from_text ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:31:40.456653Z","iopub.execute_input":"2021-12-09T03:31:40.457218Z","iopub.status.idle":"2021-12-09T03:31:40.463309Z","shell.execute_reply.started":"2021-12-09T03:31:40.457177Z","shell.execute_reply":"2021-12-09T03:31:40.462648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Classification","metadata":{}},{"cell_type":"code","source":"unique_jobs = list(data['Category'].unique())\njobs = list(data['Category'])\nresume = list(data['cleaned-1'])\nlen(resume), len(jobs)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:48.781919Z","iopub.execute_input":"2021-12-09T03:43:48.78233Z","iopub.status.idle":"2021-12-09T03:43:48.792223Z","shell.execute_reply.started":"2021-12-09T03:43:48.78228Z","shell.execute_reply":"2021-12-09T03:43:48.79123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding text","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(resume+unique_jobs)\nvocab = len(tokenizer.word_index)+1\nprint(\"Vocab Szie\".format(vocab))\n\n##Fitting the tokenizer on resumes\ntokenizer_resume = tokenizer.texts_to_sequences(resume)\nlength_list = []\nfor tokenized_seq in tokenizer_resume:\n      length_list.append(len(tokenized_seq))\nmaxlen = np.array(length_list).max()\nprint(\"resume Max Length {}\".format(maxlen))\n\npad_resume = pad_sequences(tokenizer_resume, maxlen = maxlen, padding = 'post')\npad_resume.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:48.850659Z","iopub.execute_input":"2021-12-09T03:43:48.851917Z","iopub.status.idle":"2021-12-09T03:43:50.879906Z","shell.execute_reply.started":"2021-12-09T03:43:48.851851Z","shell.execute_reply":"2021-12-09T03:43:50.878975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Encoding Categorical Labels","metadata":{}},{"cell_type":"code","source":"labelencoder = LabelEncoder()\ny = labelencoder.fit_transform(jobs)\ny_ = tf.keras.utils.to_categorical(y, 24)\ny_.shape\njobs2 = list(labelencoder.classes_)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:52.73952Z","iopub.execute_input":"2021-12-09T03:43:52.740531Z","iopub.status.idle":"2021-12-09T03:43:52.748922Z","shell.execute_reply.started":"2021-12-09T03:43:52.740468Z","shell.execute_reply":"2021-12-09T03:43:52.748159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model Implmentation","metadata":{}},{"cell_type":"code","source":"## Train test split\nX_train, X_test, y_train, y_test = train_test_split(pad_resume, y_, test_size = 0.25)\nX_train.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:43:55.500416Z","iopub.execute_input":"2021-12-09T03:43:55.501409Z","iopub.status.idle":"2021-12-09T03:43:55.540487Z","shell.execute_reply.started":"2021-12-09T03:43:55.501329Z","shell.execute_reply":"2021-12-09T03:43:55.539463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating Glove 300d embedding","metadata":{}},{"cell_type":"code","source":"embeddings_index = dict()\nfile = open('/kaggle/input/jobsextractor/glove.6B.300d.txt')\nfor line in file:\n    values=  line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\nfile.close()\n\nembedding_matrix = np.zeros((len(tokenizer.word_index)+1, 300))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\nembedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:44:13.863661Z","iopub.execute_input":"2021-12-09T00:44:13.864272Z","iopub.status.idle":"2021-12-09T00:45:05.533808Z","shell.execute_reply.started":"2021-12-09T00:44:13.864234Z","shell.execute_reply":"2021-12-09T00:45:05.533005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Applying Sequential Model keras","metadata":{}},{"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Embedding(vocab, 300, weights = [embedding_matrix], input_length=maxlen, trainable = False))\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(Dense(64, activation='relu'))\nmodel1.add(GlobalMaxPool1D())\nmodel1.add(Dense(32, activation='relu'))\nmodel1.add(Dense(24, activation='softmax'))\n\n# compile the model\nmodel1.compile(optimizer='adam', \n              loss='categorical_crossentropy', \n              metrics=['acc'])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:47:38.89094Z","iopub.execute_input":"2021-12-09T00:47:38.89148Z","iopub.status.idle":"2021-12-09T00:47:39.045315Z","shell.execute_reply.started":"2021-12-09T00:47:38.891436Z","shell.execute_reply":"2021-12-09T00:47:39.044407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:47:40.107294Z","iopub.execute_input":"2021-12-09T00:47:40.107595Z","iopub.status.idle":"2021-12-09T00:47:40.116499Z","shell.execute_reply.started":"2021-12-09T00:47:40.107555Z","shell.execute_reply":"2021-12-09T00:47:40.115849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(X_train, y_train, epochs = 50)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:47:43.640955Z","iopub.execute_input":"2021-12-09T00:47:43.641244Z","iopub.status.idle":"2021-12-09T01:00:53.374597Z","shell.execute_reply.started":"2021-12-09T00:47:43.641214Z","shell.execute_reply":"2021-12-09T01:00:53.373685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.save(\"/kaggle/working/job-categorical4.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:10:42.046061Z","iopub.execute_input":"2021-12-09T01:10:42.046554Z","iopub.status.idle":"2021-12-09T01:10:42.19047Z","shell.execute_reply.started":"2021-12-09T01:10:42.046505Z","shell.execute_reply":"2021-12-09T01:10:42.189578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\nmodel1 = load_model(\"/kaggle/input/jobsextractor/job-categorical4.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:44:41.209706Z","iopub.execute_input":"2021-12-09T03:44:41.210034Z","iopub.status.idle":"2021-12-09T03:44:41.864856Z","shell.execute_reply.started":"2021-12-09T03:44:41.209996Z","shell.execute_reply":"2021-12-09T03:44:41.863625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\nprint(\"Prediction shape\".format(y_pred.shape))\n\ny_pred_l = np.where(y_pred == y_pred[0].max(),y_pred, int(0))\ny_final_pred_l = np.where(y_pred_l != y_pred[0].max(), y_pred_l,int(1))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:27:22.758664Z","iopub.execute_input":"2021-12-09T01:27:22.759312Z","iopub.status.idle":"2021-12-09T01:27:25.121763Z","shell.execute_reply.started":"2021-12-09T01:27:22.75927Z","shell.execute_reply":"2021-12-09T01:27:25.121134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def specific_prediction(clean_text, model, tokenizer):\n    ## Converting text to tokenized sequences\n    testing = tokenizer.texts_to_sequences([clean_text])\n    testing2 = []\n    for i in testing:\n        for j in i:\n            testing2.append(j)\n    # pading the sequences to equal length\n    testing_resume = pad_sequences([testing2], maxlen = maxlen, padding = 'post')\n    pred = model.predict([testing_resume])\n    \n    ## converting prediction to text again\n    y_pred_l = np.where(pred[0]>0.4,pred[0], int(0))\n    y_final_pred_l = np.where(y_pred_l<0.4, y_pred_l,int(1))\n    y_final_pred_l\n    index = []\n    for i in list(y_pred_l):\n        if i == 0:\n            continue\n        index.append(list(y_pred_l).index(i))\n        \n    index2 = []\n    for k in  sorted(list(pred[0]), reverse = True)[:5]:\n        #print(i)\n        j = list(pred[0]).index(k)\n        index2.append(j)\n    labels = []\n    for i in index2:\n        labels.append(jobs2[i])\n    \n    ## Getting final predictions\n    pred_label = list(y_final_pred_l).index(y_final_pred_l.max())\n    label = jobs2[pred_label]\n\n    return label, labels,index2\n\nlabel, labels, i = specific_prediction(clean_text, model1, tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:08:45.913531Z","iopub.execute_input":"2021-12-09T04:08:45.914012Z","iopub.status.idle":"2021-12-09T04:08:45.997956Z","shell.execute_reply.started":"2021-12-09T04:08:45.91395Z","shell.execute_reply":"2021-12-09T04:08:45.996822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from sklearn.metrics import classification_report\n#print(classification_report(y_test, y_final_pred_l))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:09:15.28235Z","iopub.execute_input":"2021-12-09T04:09:15.283275Z","iopub.status.idle":"2021-12-09T04:09:15.287879Z","shell.execute_reply.started":"2021-12-09T04:09:15.283217Z","shell.execute_reply":"2021-12-09T04:09:15.286793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bawa = \"\"\"   EDUCATION\nGeetanjali Institute Of Technical Studies.\nB.Tech. IN COMPUTER SCIENCE\nUdaipur, India | Expected May 2022\n\nCOMPUTER SKILLS\n• MS OFFICE • SQL • HTML5 \n• System administration• WordPress \n• Windows • LINUX/UNIX\n\nTECHNICAL SKILLS\n• Python • Tensorflow • C • C++ \n• Pytorch • Keras • Matlab•Git Data structures • AWS\n\nINTERPERSONAL SKILLS\n • Analytical Thinking• Problem Solving • Technical Writing •Public speaking • Team Leading\nCERTIFICATION COURSES\nCoursera:-\n Convolutional Neural Networks in TensorFlow \nIntroduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\nProject: Custom Prediction Routine on Google AI Platform\nPython data structures\nProgramming for Everybody (Getting Started with Python)\n  Linkedin :-\nAdvance Your Skills in Deep Learning and Neural Networks\nBuilding a Recommendation System with Python Machine Learning & AI\nBuilding Deep Learning Applications with Keras 2.0\n    Udemy:-\nComplete machine learning: from zero to mastery\n   Field of Interest\nNatural Language Processing\nCognitive science\nApplied computational science\nEEG \nDeep Neural Networks\nStatistical and Mathematical Computation of Machine Learning \nFeature Engineering\n  Hobbies\nCycling and Hiking\nReading Novels\nWeight Lifting\nVolleyball \n  Links\nGithub:https://github.com/hritvikgupta\nLinkedIn: https://linkedin.com/in/hritvik-gupta-8469611a3\nGoogle scholar:\nhttps://scholar.google.com/citations?user=ShxBp2MAAAAJ&hl=en\n\n\nPROFILE SUMMARY\nResearch enthusiastic with more than two year of experience in Natural language   processing and a year of working in Electroencephalogram (EEG) signal analysis. Extensively published in computing and AI  journals. I specifically work upon customized deep neural layers and optimization functions of neural layers. Also a confident speaker at conferences and has the ability to teach coursework and complex research to all kinds of people. \n\nEXPERIENCE\nIndian Institute Of Technology,Roorkee\n|RESEARCH INTERN\n|March 2021 - October 2021| Roorkee\nDeveloping customized models using Keras to classify the EEG signals by reaction time, go/no-go and passive tasks and Analyzing the EEG signals from young and old adults based on the rest and auditory cued reaction time tasks. \nApplying several Signal Pre-processing techniques like ICA, Signal-Space Projections and Source Estimation for removing the unwanted ECG and EOG artifacts, and PCA for dimensionality reduction. \nAcademics\nBachelors of Technology Computer science Engg.\n|Geetanjali Institute of Technical Studies, Udaipur, RJ\n|8.83 CGPA                                              July 2018 - July 2022  \nHigher Secondary School \n|CBSE- Delhi Public School, Udaipur, RJ \n|85.55 %                                                July 2017- July 2018  \n\nPROJECTS\nComprehensive Analysis of the Classification of Cognitive Load Of EEG Mental Load Signals\n|In Press Research Publication|March 2021 - present\nThe motive of this research is to classify between rest-active signals and the active part of the brain bearing a considerable high load on arithmetic tasks.\nAnalyzing the EEG signals from young and old adults based on the rest and arithmetic  cued response time tasks\nEntropy, Time Domain and Frequency Domain Feature analysis.\nAblation study using neural networks .\nOutcome Frontal Lobe is most active alongside parietal lobe. \nMulti Linguistic Text Generator\n|Final Year Project |september 2021 - present\nThe aim of Multi linguistic text generators is similar to that of the Google text generator and we worked  upon deep neural improvement\nTrained on less data but running on various algorithm to recast the encoder-decoder neural networks \nOutcome supposed to be Adaptive neural networks to Multi-linguistic text embedding. \n\nUnsupervised Text Summarizer Using LSA and Sentence based     topic modeling with BERT. \n|Research Paper IEEE Publication,Summer Internship Project| july 2020-oct   2020\nThe scope of this  research project which is based on Natural language processing to Summarize the long textual document to reduce database storage size and retain only relevant information\nUsed LSA topic modeling along with TFIDF keyword extractor for each sentence in a text document \nUsed BERT for text embeddings. Coalesce all embedding to be fed to neural architecture. \nObserved considerable decrease in size of data and increased in accuracy of the trained model as compared to that of previously published \n    Hybrid Text Summarization Using Elmo Embedding. \n    |Research Paper IEEE Publication,Winter  Project| Nov 2020- Feb 2021\nThis research project aims to build the algorithm to analyse unsupervised embedding when incorporate with supervised approach of ranking sentences\nText summarizer is built combining ELmo based text embedding which is unsupervised to the supervised approach of cosine similarity to build an efficient text summarizer. \nOutcome is a considerable increase in ROUGE-1 and ROUGE-L score is observed as compared to that of the previously published results on similar dataset. \n  Image Captioning\n  |Minor Project | June 2021            \nThis is one of the projects that I have built during the first 2 months of my internship at IIT while learning mathematical computation of keras neural networks. This includes creating an Image array using Res-Net model then building its own Custom Keras Lstm model for generating captions.\n\n     PUBLICATIONS \nH. Gupta and M. Patel, \"Method Of Text Summarization Using Lsa And Sentence Based Topic Modelling With Bert,\" 2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS), 2021, pp. 511-517, doi: 10.1109/ICAIS50930.2021.9395976.\n\nH. Gupta and M. Patel, \"Study of Extractive Text Summarizer Using The Elmo Embedding,\" 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 2020, pp. 829-834, doi: 10.1109/I-SMAC49090.2020.924361\n\n       In Peer Publications\nAnalysing of EEG Signals Using RNN Classification\n|IEEE Scopus Journal |  November 25-27 2021   \n  \nComprehensive Analysis of the Classification of Cognitive Load Of EEG Mental Load Signals \n             |MDPI MOCAST |  January 15 -16 2022\n           \n     CONFERENCE PRESENTATIONS\n\n3rd International conference on innovations in power and advanced computing technologies I-PACT \n             | November 25-27 2021  \n\nInternational Conference on Artificial Intelligence and Smart Systems (ICAIS) \n | March 26-27 2021  \n\n4th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)\n             |November 25-27 2020  \n\n         LEADERSHIP AND VOLUNTEER EXPERIENCE\n\nHome Town Free Food Service \n|August 2020 - Present\n\nA part-time worker in an NGO which aims to provide free food to the Poor and necessities people in times of covid crisis. My job is to locate these people in places like bus stations, railway stations and certain sub-rural places.\n\nStudent Technical Club\n|September 2019 - Present\n\nLeader of the AI and AR/VR team in the technical club of Geetanjali institutes computer science which aims to enhance the technical skills of students in all fields and each respective field has a team and a team leader which specializes in that field.\n*\n\n\n\n\n\n\n\n\n\n\n\n\"\"\"\ndocter = \"\"\"PRAYANSH\nMAHESHWARI\nCONTACT DETAILS\nInfotainment Head Unit - User Engagement Dashboard\nThe aim of this project was to track user engagement in order to curate or\ncustomize the programs available and detect every deployment issue\nWorked on SQL(Hive) to extract and process client data and derived useful\ninsights to be included in Dashboard (Tableau)\nBuilt a dashboard which enabled Product, Marketing and Customer Experience\nTeam to take data-driven decision together\nCustomer Churn Management - Offer prioritization\nImplemented classification offer prioritization model to identify customer's\npropensity towards buying an offer and providing insights for different\nsegments of customers based on their behaviour\nThis resulted in a decrease in churn rate by ~2%\nIn addition, it helped the client realize that customers can be saved at a\nhigher-priced offer. $3M annualized revenue was realized from the project\nOrder Fulfillment - Request Management Portal\nThe aim of this project was to build a Report Management Tool which will\ndecrease the manual interventions by creating a web application tool\nThis resulted in saving 15 hrs. of work per week for 3 admin users. The tool is\nalso utilized by 2000+ users.\nMu Sigma\nTrainee Decision Scientist (June 2018 - May 2020)\nNirma University, ITNU\nB.Tech in Electronics and\nCommunication\nScored - 7.61 CGPA\nDAV School, Ajmer\nCBSE - Higher Secondary\nScored - 94%\nMayoor School, Ajmer\nCBSE - Senior Secondary\nScored - 9.8 CGPA\nEDUCATION\nSKILLS\nACHIVEMENTS\nSpot Award\nGreat job done at re-wiring the RMP\napplication components to make it\npresentable in a short turnaround time.\nHelping out your teammates around\nthe technical difficulties was\ncommendable.\nMentored a group of four inductees to guide them through problem space\njourney and provide actionable feedback for their skills enhancement\nVolunteered in M.A.D(Make a Difference) - Non-profit organization, working to\nensure better outcomes for children in orphanages and shelters across India\nOther interests - Cricket, Football, Adventure Sports and Painting\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:47:25.975753Z","iopub.execute_input":"2021-12-09T03:47:25.97608Z","iopub.status.idle":"2021-12-09T03:47:25.985796Z","shell.execute_reply.started":"2021-12-09T03:47:25.976046Z","shell.execute_reply":"2021-12-09T03:47:25.985024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    t = clean_t(docter)\n    t2 = clean_data(t)\n    review = re.sub(\n        '(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\n        \" \",\n        docter,\n    )\n    review = review.lower()\n    review = review.split()\n    lm = WordNetLemmatizer()\n    review = [ lm.lemmatize(word) for word in review if not word in set(stopwords)]\n    review = \" \".join(review)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:34.601767Z","iopub.execute_input":"2021-12-09T03:52:34.602617Z","iopub.status.idle":"2021-12-09T03:52:34.617239Z","shell.execute_reply.started":"2021-12-09T03:52:34.602569Z","shell.execute_reply":"2021-12-09T03:52:34.616455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = nlp(review)\nfor i in e.ents:\n    print(i.text)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:38.138145Z","iopub.execute_input":"2021-12-09T03:52:38.139091Z","iopub.status.idle":"2021-12-09T03:52:38.205632Z","shell.execute_reply.started":"2021-12-09T03:52:38.139042Z","shell.execute_reply":"2021-12-09T03:52:38.204326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing = tokenizer.texts_to_sequences([t2])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:46.430959Z","iopub.execute_input":"2021-12-09T03:52:46.431897Z","iopub.status.idle":"2021-12-09T03:52:46.436199Z","shell.execute_reply.started":"2021-12-09T03:52:46.431842Z","shell.execute_reply":"2021-12-09T03:52:46.43549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:48.085317Z","iopub.execute_input":"2021-12-09T03:52:48.085643Z","iopub.status.idle":"2021-12-09T03:52:48.091169Z","shell.execute_reply.started":"2021-12-09T03:52:48.085612Z","shell.execute_reply":"2021-12-09T03:52:48.090112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing2 = []\nfor i in testing:\n    for j in i:\n        testing2.append(j)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:48.790234Z","iopub.execute_input":"2021-12-09T03:52:48.790604Z","iopub.status.idle":"2021-12-09T03:52:48.797514Z","shell.execute_reply.started":"2021-12-09T03:52:48.790566Z","shell.execute_reply":"2021-12-09T03:52:48.796484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing2","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:50.08355Z","iopub.execute_input":"2021-12-09T03:52:50.083857Z","iopub.status.idle":"2021-12-09T03:52:50.08863Z","shell.execute_reply.started":"2021-12-09T03:52:50.083826Z","shell.execute_reply":"2021-12-09T03:52:50.08772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_resume = pad_sequences([testing2], maxlen = maxlen, padding = 'post')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:50.590793Z","iopub.execute_input":"2021-12-09T03:52:50.591673Z","iopub.status.idle":"2021-12-09T03:52:50.596623Z","shell.execute_reply.started":"2021-12-09T03:52:50.591629Z","shell.execute_reply":"2021-12-09T03:52:50.595549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = model1.predict([testing_resume])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:51.94621Z","iopub.execute_input":"2021-12-09T03:52:51.946934Z","iopub.status.idle":"2021-12-09T03:52:52.027088Z","shell.execute_reply.started":"2021-12-09T03:52:51.946865Z","shell.execute_reply":"2021-12-09T03:52:52.025644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:53.862717Z","iopub.execute_input":"2021-12-09T03:52:53.863615Z","iopub.status.idle":"2021-12-09T03:52:53.870796Z","shell.execute_reply.started":"2021-12-09T03:52:53.86356Z","shell.execute_reply":"2021-12-09T03:52:53.869829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_resume.shape","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:52:54.410562Z","iopub.execute_input":"2021-12-09T03:52:54.411294Z","iopub.status.idle":"2021-12-09T03:52:54.418216Z","shell.execute_reply.started":"2021-12-09T03:52:54.41124Z","shell.execute_reply":"2021-12-09T03:52:54.417189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_l = np.where(p[0]>0.4,p[0], int(0))\ny_final_pred_l = np.where(y_pred_l<0.4, y_pred_l,int(1))\ny_final_pred_l\nindex = []\nfor i in list(y_pred_l):\n    if i == 0:\n        continue\n    index.append(list(y_pred_l).index(i))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:52:55.046223Z","iopub.execute_input":"2021-12-09T03:52:55.047107Z","iopub.status.idle":"2021-12-09T03:52:55.052828Z","shell.execute_reply.started":"2021-12-09T03:52:55.047048Z","shell.execute_reply":"2021-12-09T03:52:55.052145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(y_final_pred_l).index(y_final_pred_l.max())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:56.475718Z","iopub.execute_input":"2021-12-09T03:52:56.476436Z","iopub.status.idle":"2021-12-09T03:52:56.484267Z","shell.execute_reply.started":"2021-12-09T03:52:56.476365Z","shell.execute_reply":"2021-12-09T03:52:56.483196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in list(y_final_pred_l):\n    if i == 1:\n        print(list(y_final_pred_l).index(i))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:52:57.6726Z","iopub.execute_input":"2021-12-09T03:52:57.67292Z","iopub.status.idle":"2021-12-09T03:52:57.679688Z","shell.execute_reply.started":"2021-12-09T03:52:57.672889Z","shell.execute_reply":"2021-12-09T03:52:57.678763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_l","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:52:58.487947Z","iopub.execute_input":"2021-12-09T03:52:58.488442Z","iopub.status.idle":"2021-12-09T03:52:58.494686Z","shell.execute_reply.started":"2021-12-09T03:52:58.488403Z","shell.execute_reply":"2021-12-09T03:52:58.493831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(p[0]).index(p[0].max())","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-09T03:48:05.427284Z","iopub.execute_input":"2021-12-09T03:48:05.427719Z","iopub.status.idle":"2021-12-09T03:48:05.43538Z","shell.execute_reply.started":"2021-12-09T03:48:05.427663Z","shell.execute_reply":"2021-12-09T03:48:05.434482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jobs2[5]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T03:48:09.180375Z","iopub.execute_input":"2021-12-09T03:48:09.180902Z","iopub.status.idle":"2021-12-09T03:48:09.186683Z","shell.execute_reply.started":"2021-12-09T03:48:09.180845Z","shell.execute_reply":"2021-12-09T03:48:09.185808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in index:\n    print(jobs2[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(data[data['Category'] == 'ENGINEERING']['Resume_str'])[117]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eng = \"\"\"HARSHIT PALIWAL\nC O M P U T E R S C I E N C E S T U D E N T\nHello, my name is Harshit Paliwal. I’m\nstudying computer science engineering\nbecause I’m passionate about exploring ways\ntechnology can provide practical solutions to\neveryday problems.\nCONTACT DETAILS\nMobile: 8239251295\nEmail: harshitpaliwal95@gmail.com\nLinkedin: harshit-paliwal\nTwitter: harshit_hp\nGithub - harshitpaliwal95\nPortfolio - harshit-paliwal.netlify.app/\nEDUCATION\nB.tech\nGeetanjali Institute of Technical Studies,\nUdaipur\nCOMPUTER SCIENCE ENGINEERING\n6.2 CGPA | 2018 - present\n12th\nAlok Senior Secondary School, Udaipur\n51% | 2017 - 2018\nPROJECTS\nPortfolio site\n- build with Html Css & JavaScript || live\nDice game\n- build a dice game for 2 player logic build on Javascript\n- UI build on Html CSS code || live\nNaruto Game\n- build anime game that compares characters power\n-logic build on javaScript\n-UI build on Html Scss || live\n10th\nVidhya Bharti Senior Secondary School,\nUdaipur\n66% | 2015 - 2016\nSKILLS\nProgramming languages :\nc/c++, Java, JavaScript\nWeb Technologies:\nHTML5, CSS3, SCSS, BOOTSTRAP, REACTJS\nTools:\nGit, Github, Linux, Vs code, Command Line\nINTERESTS\nOpen Source Contribution\nLearn New Technology\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}